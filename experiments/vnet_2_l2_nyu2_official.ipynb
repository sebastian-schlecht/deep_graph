{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 90.0% of memory, cuDNN Version is too old. Update to v5, was 2000.)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from deepgraph.utils.logging import log\n",
    "from deepgraph.utils.common import batch_parallel, ConfigMixin, shuffle_in_unison_inplace, pickle_dump\n",
    "from deepgraph.utils.image import batch_pad_mirror\n",
    "from deepgraph.constants import *\n",
    "from deepgraph.conf import rng\n",
    "\n",
    "from deepgraph.pipeline import Processor, Packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepgraph.nn.init import *\n",
    "from deepgraph.node import Node\n",
    "import h5py, numpy as np\n",
    "\n",
    "# Custom loss node\n",
    "class MaskedLogLoss(Node):\n",
    "    \"\"\"\n",
    "    Compute log scale invariant error for depth prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, graph, name, config={}):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param graph: Graph\n",
    "        :param name: String\n",
    "        :param config: Dict\n",
    "        :return: Node\n",
    "        \"\"\"\n",
    "        super(MaskedLogLoss, self).__init__(graph, name, config=config)\n",
    "        self.is_loss = True\n",
    "\n",
    "    def setup_defaults(self):\n",
    "        super(MaskedLogLoss, self).setup_defaults()\n",
    "        self.conf_default(\"loss_weight\", 1.0)\n",
    "        self.conf_default(\"lambda\", 0.5)\n",
    "        self.conf_default(\"label_index\", 1)\n",
    "\n",
    "    def alloc(self):\n",
    "            if len(self.inputs) != 2:\n",
    "                raise AssertionError(\"This node needs exactly two inputs to calculate loss.\")\n",
    "            self.output_shape = (1,)\n",
    "\n",
    "    def forward(self):\n",
    "            # Define our forward function\n",
    "            if self.conf(\"label_index\") == 1:\n",
    "                pred = self.inputs[0].expression\n",
    "                target = self.inputs[1].expression\n",
    "            else:\n",
    "                pred = self.inputs[1].expression\n",
    "                target = self.inputs[0].expression\n",
    "            \n",
    "            # Compute a mask which removes invalid values or 0\n",
    "            null_mask = T.gt(target,0)\n",
    "            # Compute a mask which removes the max values of a depth image\n",
    "            max_mask = T.lt(target, T.max(target))\n",
    "            # Mask out min\n",
    "            min_mask = T.gt(target, T.min(target))\n",
    "            # Combine masks\n",
    "            mask = null_mask * max_mask * min_mask\n",
    "            # Now we compute the log of the input data stream, but neglect invalid values\n",
    "            log_target = T.switch(mask, T.log(target),0)\n",
    "            # We also apply the mask to our predictions in order to avoid computation of gradients on invalid pixels\n",
    "            masked_pred = mask * pred\n",
    "            # We compute the difference here to construct l_2 norm later in log space, neglecting invalid pixels\n",
    "            diff = log_target - masked_pred\n",
    "            # We also have to compute the mean only on valid pixels, so we need the number of valid pixels\n",
    "            n_valid = mask.sum()\n",
    "            # Finally compute the scale invariant error \n",
    "            self.expression = T.sum(diff**2) / n_valid - ((self.conf(\"lambda\") / (n_valid**2)) * (T.sum(diff)**2))\n",
    "            \n",
    "            \n",
    "\n",
    "class NYUPrefetcher(Processor):\n",
    "    \"\"\"\n",
    "    Processor class to asynchronously load data in chunks and prepare it for later stages\n",
    "    \"\"\"\n",
    "    def __init__(self, name, shapes, config, buffer_size=10):\n",
    "        super(NYUPrefetcher, self).__init__(name, shapes, config, buffer_size)\n",
    "        self.db_handle = None\n",
    "        self.cursor = 0\n",
    "        self.data_field = None\n",
    "        self.label_field = None\n",
    "        self.thresh = 0\n",
    "\n",
    "    def setup_defaults(self):\n",
    "        super(NYUPrefetcher, self).setup_defaults()\n",
    "        self.conf_default(\"db\", None)\n",
    "        self.conf_default(\"key_label_train\", \"train_depths\")\n",
    "        self.conf_default(\"key_data_train\", \"train_images\")\n",
    "        self.conf_default(\"key_label_test\", \"test_depth\")\n",
    "        self.conf_default(\"key_data_test\", \"test_images\")\n",
    "        self.conf_default(\"chunk_size\", 320)\n",
    "\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        Open a handle to the database and check if the config is valid and files exist\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        log(\"NYUPrefetcher - Caching DB in memory\", LOG_LEVEL_INFO)\n",
    "        assert self.conf(\"db\") is not None\n",
    "        self.db_handle = h5py.File(self.conf(\"db\"))\n",
    "        \n",
    "        self.data_field_train = np.array(self.db_handle[self.conf(\"key_data_train\")])\n",
    "        self.label_field_train = np.array(self.db_handle[self.conf(\"key_label_train\")])\n",
    "        \n",
    "        self.data_field_test = np.array(self.db_handle[self.conf(\"key_data_test\")])\n",
    "        self.label_field_test = np.array(self.db_handle[self.conf(\"key_label_test\")])\n",
    "        \n",
    "        assert self.data_field_train.shape[0] == self.label_field_train.shape[0]\n",
    "        assert self.data_field_test.shape[0] == self.label_field_test.shape[0]\n",
    "\n",
    "        self.db_handle.close()\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Read elements from the database in packes specified by \"chunk_size\". Feed each packet to the top processor\n",
    "        in case its entry queue is not full\n",
    "        :return: Bool\n",
    "        \"\"\"\n",
    "        if self.top is not None:\n",
    "            packet = self.pull()\n",
    "            if packet is None:\n",
    "                return False\n",
    "            log(\"H5DBLoader - Preloading chunk\", LOG_LEVEL_VERBOSE)\n",
    "            start = time.time()\n",
    "            if packet.phase == PHASE_TRAIN:\n",
    "                c_size = self.conf(\"chunk_size\")\n",
    "                upper = min(self.data_field_train.shape[0], self.cursor + c_size)\n",
    "                data = self.data_field_train[self.cursor:upper].copy()\n",
    "                label = self.label_field_train[self.cursor:upper].copy()\n",
    "\n",
    "                self.cursor += c_size\n",
    "                # Reset cursor in case we exceeded the array ranges\n",
    "                if self.cursor > self.data_field_train.shape[0]:\n",
    "                    self.cursor = 0\n",
    "\n",
    "            # Load entire validation data for now (one val cycle)\n",
    "            elif packet.phase == PHASE_VAL:\n",
    "                data = self.data_field_test.copy()\n",
    "                label = self.label_field_test.copy()\n",
    "            # End phase or unknown\n",
    "            else:\n",
    "                data, label = (None, None)\n",
    "\n",
    "            end = time.time()\n",
    "            log(\"H5DBLoader - Fetching took \" + str(end - start) + \" seconds.\", LOG_LEVEL_VERBOSE)\n",
    "            self.push(Packet(identifier=packet.id,\n",
    "                             phase=packet.phase,\n",
    "                             num=2,\n",
    "                             data=(data, label)))\n",
    "            return True\n",
    "        else:\n",
    "            # No top node found, enter spin wait time\n",
    "            return False\n",
    "# Input data transformer\n",
    "class Transformer(Processor):\n",
    "    \"\"\"\n",
    "    Apply online random augmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, shapes, config, buffer_size=10):\n",
    "        super(Transformer, self).__init__(name, shapes, config, buffer_size)\n",
    "        self.mean = None\n",
    "\n",
    "    def init(self):\n",
    "        if self.conf(\"mean_file\") is not None:\n",
    "            self.mean = np.load(self.conf(\"mean_file\"))\n",
    "        else:\n",
    "            log(\"Transformer - No mean file specified.\", LOG_LEVEL_WARNING)\n",
    "\n",
    "    def process(self):\n",
    "        packet = self.pull()\n",
    "        # Return if no data is there\n",
    "        if not packet:\n",
    "            return False\n",
    "        # Unpack\n",
    "        data, label = packet.data\n",
    "        # Do processing\n",
    "        log(\"Transformer - Processing data\", LOG_LEVEL_VERBOSE)\n",
    "        \n",
    "        h = 240\n",
    "        w = 320\n",
    "        \n",
    "        start = time.time()\n",
    "        # Mean\n",
    "        if packet.phase == PHASE_TRAIN or packet.phase == PHASE_VAL:\n",
    "            data = data.astype(np.float32)\n",
    "            if self.mean is not None:\n",
    "                std = self.conf(\"std_norm\")\n",
    "                for idx in range(data.shape[0]):\n",
    "                    # Subtract mean\n",
    "                    data[idx] = data[idx] - self.mean.astype(np.float32)\n",
    "                    if std is not None:\n",
    "                        data[idx] =  data[idx] * std\n",
    "\n",
    "            if self.conf(\"offset\") is not None:\n",
    "                label -= self.conf(\"offset\")\n",
    "\n",
    "        if packet.phase == PHASE_TRAIN:\n",
    "             # Do elementwise operations\n",
    "            data_old = data\n",
    "            label_old = label\n",
    "            data = np.zeros((data_old.shape[0], data_old.shape[1], h, w), dtype=np.float32)\n",
    "            label = np.zeros((label_old.shape[0], h, w), dtype=np.float32)\n",
    "            for idx in range(data.shape[0]):\n",
    "                # Rotate\n",
    "                # We rotate before cropping to be able to get filled corners\n",
    "                # Maybe even adjust the border after rotating\n",
    "                deg = np.random.randint(-5,6)\n",
    "                # Operate on old data. Careful - data is already in float so we need to normalize and rescale afterwards\n",
    "                # data_old[idx] = 255. * rotate_transformer_rgb_uint8(data_old[idx] * 0.003921568627, deg).astype(np.float32)\n",
    "                # label_old[idx] = rotate_transformer_scalar_float32(label_old[idx], deg)\n",
    "                \n",
    "                # Take care of any empty areas, we crop on a smaller surface depending on the angle\n",
    "                # TODO Remove this once loss supports masking\n",
    "\n",
    "                shift = 0 #np.tan((deg/180.) * math.pi)\n",
    "                # Random crops\n",
    "                cy = rng.randint(data_old.shape[2] - h - shift, size=1)\n",
    "                cx = rng.randint(data_old.shape[3] - w - shift, size=1)\n",
    "\n",
    "                data[idx] = data_old[idx, :, cy:cy+h, cx:cx+w]\n",
    "                label[idx] = label_old[idx, cy:cy+h, cx:cx+w]\n",
    "\n",
    "                # Flip horizontally with probability 0.5\n",
    "                p = rng.randint(2)\n",
    "                if p > 0:\n",
    "                    data[idx] = data[idx, :, :, ::-1]\n",
    "                    label[idx] = label[idx, :, ::-1]\n",
    "\n",
    "                # RGB we mult with a random value between 0.8 and 1.2\n",
    "                r = rng.randint(80,121) / 100.\n",
    "                g = rng.randint(80,121) / 100.\n",
    "                b = rng.randint(80,121) / 100.\n",
    "                data[idx, 0] = data[idx, 0] * r\n",
    "                data[idx, 1] = data[idx, 1] * g\n",
    "                data[idx, 2] = data[idx, 2] * b\n",
    "                \n",
    "            # Shuffle\n",
    "            data, label = shuffle_in_unison_inplace(data, label)\n",
    "            \n",
    "        elif packet.phase == PHASE_VAL:\n",
    "            # Center crop\n",
    "            cy = (data.shape[2] - h) // 2\n",
    "            cx = (data.shape[3] - w) // 2\n",
    "            data = data[:, :, cy:cy+h, cx:cx+w]\n",
    "            label = label[:, cy:cy+h, cx:cx+w]\n",
    "            \n",
    "        end = time.time()\n",
    "        log(\"Transformer - Processing took \" + str(end - start) + \" seconds.\", LOG_LEVEL_VERBOSE)\n",
    "        # Try to push into queue as long as thread should not terminate\n",
    "        self.push(Packet(identifier=packet.id, phase=packet.phase, num=2, data=(data, label)))\n",
    "        return True\n",
    "\n",
    "    def setup_defaults(self):\n",
    "        super(Transformer, self).setup_defaults()\n",
    "        self.conf_default(\"mean_file\", None)\n",
    "        self.conf_default(\"offset\", None)\n",
    "        self.conf_default(\"std_norm\", 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DeepGraph Neural Network API for Theano\n",
      "\n",
      "Available on GitHub: https://github.com/sebastian-schlecht/deepgraph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import relu\n",
    "\n",
    "from deepgraph.graph import *\n",
    "from deepgraph.nn.core import *\n",
    "from deepgraph.nn.conv import *\n",
    "from deepgraph.nn.loss import *\n",
    "from deepgraph.solver import *\n",
    "from deepgraph.nn.init import *\n",
    "\n",
    "from deepgraph.pipeline import Optimizer, H5DBLoader, Pipeline\n",
    "\n",
    "\n",
    "def build_u_graph():\n",
    "    # Disable CUDNN\n",
    "    # TODO Remove\n",
    "    Conv2D.use_cudnn = False\n",
    "    Pool.use_cudnn = False\n",
    "    \n",
    "    graph = Graph(\"u_depth\")\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    \"\"\"\n",
    "    data = Data(graph, \"data\", T.ftensor4, shape=(-1, 3, 240, 320))\n",
    "    label = Data(graph, \"label\", T.ftensor3, shape=(-1, 1, 240, 320), config={\n",
    "        \"phase\": PHASE_TRAIN\n",
    "    })\n",
    "    \"\"\"\n",
    "    Contractive part\n",
    "    \"\"\"\n",
    "    conv_1 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_1\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_2 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_2\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    pool_2 = Pool(graph, \"pool_2\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_3 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_3\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_4 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_4\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    pool_4 = Pool(graph, \"pool_4\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "\n",
    "    conv_5 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_5\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_6 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_6\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    pool_6 = Pool(graph, \"pool_6\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "\n",
    "    conv_7 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_7\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_8 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_8\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    dp_c8  = Dropout(graph, \"dpc_8\")\n",
    "    pool_8 = Pool(graph, \"pool_8\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    fl = Flatten(graph, \"fl\",config={\n",
    "            \"dims\": 2\n",
    "    })\n",
    "    fc_8 = Dense(graph, \"fc_8\", config={\n",
    "        \"out\": 4096,\n",
    "        \"activation\": relu,\n",
    "        \"weight_filler\": xavier(),\n",
    "        \"bias_filler\": constant(0.01)\n",
    "    })\n",
    "    dp_8 = Dropout(graph, \"dp_8\")\n",
    "    fc_9 = Dense(graph, \"fc_9\", config={\n",
    "        \"out\": 19200,\n",
    "        \"activation\": relu,\n",
    "        \"weight_filler\": xavier(),\n",
    "        \"bias_filler\": constant(0.01)\n",
    "    })\n",
    "    dp_9 = Dropout(graph, \"dp_9\")\n",
    "    rs_10 = Reshape(graph, \"rs_10\", config={\n",
    "        \"shape\": (-1, 64, 15, 20)\n",
    "    })\n",
    "    \"\"\"\n",
    "    Expansive path\n",
    "    \"\"\"\n",
    "    up_11 = Upsample(graph, \"up_11\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_11 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_11\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_12 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_12\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_13 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_13\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    up_14 = Upsample(graph, \"up_14\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_14 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_14\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_15 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_15\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_16 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_16\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    up_17 = Upsample(graph, \"up_17\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_17 = Conv2D(graph, \"conv_17\", config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "    })\n",
    "    conv_18 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_18\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_19 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_19\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    up_20 = Upsample(graph, \"up_20\", config={\n",
    "        \"mode\": \"constant\",\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_20 = Conv2D(graph, \"conv_20\", config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "    })\n",
    "    conv_21 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_21\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_22 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_22\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_23 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_23_with_bias\",\n",
    "        config={\n",
    "            \"channels\": 1,\n",
    "            \"kernel\": (1, 1),\n",
    "            \"activation\": None,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0),\n",
    "            \"is_output\": True\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Feed forward nodes\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    concat_20 = Concatenate(graph, \"concat_20\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    concat_17 = Concatenate(graph, \"concat_17\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "    concat_14 = Concatenate(graph, \"concat_14\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    concat_11 = Concatenate(graph, \"concat_11\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Losses / Error\n",
    "    \"\"\"\n",
    "    loss = MaskedLogLoss(graph, \"loss\",config={\"lambda\": 0})\n",
    "\n",
    "    project = Function(graph, \"proj_log\", config={\n",
    "        \"expression\": lambda x: T.exp(x),\n",
    "        \"phase\": PHASE_TRAIN\n",
    "    })\n",
    "    error = MSE(graph, \"rmse\", config={\n",
    "        \"root\": True,\n",
    "        \"is_output\": True,\n",
    "        \"phase\": PHASE_TRAIN\n",
    "    })\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Make connections\n",
    "    \"\"\"\n",
    "    data.connect(conv_1)\n",
    "    conv_1.connect(conv_2)\n",
    "    conv_2.connect(concat_20)\n",
    "    conv_2.connect(pool_2)\n",
    "    pool_2.connect(conv_3)\n",
    "    conv_3.connect(conv_4)\n",
    "    conv_4.connect(concat_17)\n",
    "    conv_4.connect(pool_4)\n",
    "    pool_4.connect(conv_5)\n",
    "    conv_5.connect(conv_6)\n",
    "    conv_6.connect(concat_14)\n",
    "    conv_6.connect(pool_6)\n",
    "    pool_6.connect(conv_7)\n",
    "    conv_7.connect(conv_8)\n",
    "    conv_8.connect(concat_11)\n",
    "    conv_8.connect(dp_c8)\n",
    "    dp_c8.connect(pool_8)\n",
    "    pool_8.connect(fl)\n",
    "    fl.connect(fc_8)\n",
    "    fc_8.connect(dp_8)\n",
    "    dp_8.connect(fc_9)\n",
    "    fc_9.connect(dp_9)\n",
    "    dp_9.connect(rs_10)\n",
    "    rs_10.connect(up_11)\n",
    "    up_11.connect(conv_11)\n",
    "    conv_11.connect(concat_11)\n",
    "    concat_11.connect(conv_12)\n",
    "    conv_12.connect(conv_13)\n",
    "    conv_13.connect(up_14)\n",
    "    up_14.connect(conv_14)\n",
    "    conv_14.connect(concat_14)\n",
    "    concat_14.connect(conv_15)\n",
    "    conv_15.connect(conv_16)\n",
    "    conv_16.connect(up_17)\n",
    "    up_17.connect(conv_17)\n",
    "    conv_17.connect(concat_17)\n",
    "    concat_17.connect(conv_18)\n",
    "    conv_18.connect(conv_19)\n",
    "    conv_19.connect(up_20)\n",
    "    up_20.connect(conv_20)\n",
    "    conv_20.connect(concat_20)\n",
    "    concat_20.connect(conv_21)\n",
    "    conv_21.connect(conv_22)\n",
    "    conv_22.connect(conv_23)\n",
    "    \n",
    "    # Make sure we connect the prediction first and then the loss to compute the log space properly\n",
    "    # -> Pred index defaults to 0 for masked log loss\n",
    "    conv_23.connect(loss)\n",
    "    label.connect(loss)\n",
    "\n",
    "    conv_23.connect(project)\n",
    "    label.connect(error)\n",
    "    project.connect(error)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-05-07 16:40:08] INFO: NYUPrefetcher - Caching DB in memory\n",
      "[2016-05-07 16:40:09] INFO: Graph - Setting up graph\n",
      "[2016-05-07 16:40:09] INFO: Node - data has shape (-1, 3, 240, 320)\n",
      "[2016-05-07 16:40:09] INFO: Node - label has shape (-1, 1, 240, 320)\n",
      "[2016-05-07 16:40:09] INFO: Node - conv_1 has shape (-1, 64, 240, 320)\n",
      "[2016-05-07 16:40:11] INFO: Node - conv_2 has shape (-1, 64, 240, 320)\n",
      "[2016-05-07 16:40:11] INFO: Node - pool_2 has shape (-1, 64, 120, 160)\n",
      "[2016-05-07 16:40:11] INFO: Node - conv_3 has shape (-1, 128, 120, 160)\n",
      "[2016-05-07 16:40:11] INFO: Node - conv_4 has shape (-1, 128, 120, 160)\n",
      "[2016-05-07 16:40:11] INFO: Node - pool_4 has shape (-1, 128, 60, 80)\n",
      "[2016-05-07 16:40:11] INFO: Node - conv_5 has shape (-1, 256, 60, 80)\n",
      "[2016-05-07 16:40:11] INFO: Node - conv_6 has shape (-1, 256, 60, 80)\n",
      "[2016-05-07 16:40:11] INFO: Node - pool_6 has shape (-1, 256, 30, 40)\n",
      "[2016-05-07 16:40:12] INFO: Node - conv_7 has shape (-1, 512, 30, 40)\n",
      "[2016-05-07 16:40:13] INFO: Node - conv_8 has shape (-1, 512, 30, 40)\n",
      "[2016-05-07 16:40:13] INFO: Node - dpc_8 has shape (-1, 512, 30, 40)\n",
      "[2016-05-07 16:40:13] INFO: Node - pool_8 has shape (-1, 512, 15, 20)\n",
      "[2016-05-07 16:40:13] INFO: Node - fl has shape (-1, 153600)\n",
      "[2016-05-07 16:40:43] INFO: Node - fc_8 has shape (-1, 4096)\n",
      "[2016-05-07 16:40:43] INFO: Node - dp_8 has shape (-1, 4096)\n",
      "[2016-05-07 16:40:46] INFO: Node - fc_9 has shape (-1, 19200)\n",
      "[2016-05-07 16:40:46] INFO: Node - dp_9 has shape (-1, 19200)\n",
      "[2016-05-07 16:40:46] INFO: Node - rs_10 has shape (-1, 64, 15, 20)\n",
      "[2016-05-07 16:40:46] INFO: Node - up_11 has shape (-1, 64, 30, 40)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_11 has shape (-1, 512, 30, 40)\n",
      "[2016-05-07 16:40:46] INFO: Node - concat_11 has shape (-1, 1024, 30, 40)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_12 has shape (-1, 512, 30, 40)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_13 has shape (-1, 512, 30, 40)\n",
      "[2016-05-07 16:40:46] INFO: Node - up_14 has shape (-1, 512, 60, 80)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_14 has shape (-1, 256, 60, 80)\n",
      "[2016-05-07 16:40:46] INFO: Node - concat_14 has shape (-1, 512, 60, 80)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_15 has shape (-1, 256, 60, 80)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_16 has shape (-1, 256, 60, 80)\n",
      "[2016-05-07 16:40:46] INFO: Node - up_17 has shape (-1, 256, 120, 160)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_17 has shape (-1, 128, 120, 160)\n",
      "[2016-05-07 16:40:46] INFO: Node - concat_17 has shape (-1, 256, 120, 160)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_18 has shape (-1, 128, 120, 160)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_19 has shape (-1, 128, 120, 160)\n",
      "[2016-05-07 16:40:46] INFO: Node - up_20 has shape (-1, 128, 240, 320)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_20 has shape (-1, 64, 240, 320)\n",
      "[2016-05-07 16:40:46] INFO: Node - concat_20 has shape (-1, 128, 240, 320)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_21 has shape (-1, 64, 240, 320)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_22 has shape (-1, 64, 240, 320)\n",
      "[2016-05-07 16:40:46] INFO: Node - conv_23_with_bias has shape (-1, 1, 240, 320)\n",
      "[2016-05-07 16:40:46] INFO: Node - loss has shape (1,)\n",
      "[2016-05-07 16:40:46] INFO: Node - proj_log has shape (-1, 1, 240, 320)\n",
      "[2016-05-07 16:40:46] INFO: Node - rmse has shape (1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastianschlecht/anaconda2/envs/deep/lib/python2.7/site-packages/ipykernel/__main__.py:58: DeprecationWarning: Division of two integer types with x / y is deprecated, please use x // y for an integer division.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-05-07 16:40:51] INFO: Graph - Invoking Theano compiler\n",
      "[2016-05-07 16:41:04] INFO: Optimizer - Compilation finished\n",
      "[2016-05-07 16:41:05] INFO: Optimizer - Warmup starting. Changing LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastianschlecht/anaconda2/envs/deep/lib/python2.7/site-packages/ipykernel/__main__.py:211: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "/home/sebastianschlecht/anaconda2/envs/deep/lib/python2.7/site-packages/ipykernel/__main__.py:212: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    batch_size = 4\n",
    "    chunk_size = 100\n",
    "    transfer_shape = ((chunk_size, 3, 240, 320), (chunk_size, 240, 320))\n",
    "\n",
    "    g = build_u_graph()\n",
    "\n",
    "    # Build the training pipeline\n",
    "    db_loader = NYUPrefetcher(\"db\", ((chunk_size, 3, 480, 640), (chunk_size, 1, 480, 640)), config={\n",
    "        \"db\": \"/media/data/depth_data/nyu_depth_v2_official.mat\",\n",
    "        \"key_data_train\": \"train_images\",\n",
    "        \"key_label_train\": \"train_depths\",\n",
    "        \"key_data_test\": \"test_images\",\n",
    "        \"key_label_test\": \"test_depths\",\n",
    "        \"chunk_size\": chunk_size\n",
    "    })\n",
    "    transformer = Transformer(\"tr\", transfer_shape, config={\n",
    "        # Pixel & channelwise mean \n",
    "        \"mean_file\": \"/media/data/depth_data/nyu_depth_v2_official.npy\",\n",
    "        # 1 / sqrt(var(data))\n",
    "        \"std_norm\": 1.0 / 72.240064849107824\n",
    "    })\n",
    "    optimizer = Optimizer(\"opt\", g, transfer_shape, config={\n",
    "        \"batch_size\":  batch_size,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"learning_rate\": 0.05, # for step 1, with warmup\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.0005,\n",
    "        \"print_freq\": 50,\n",
    "        \"lr_policy\": \"step\",\n",
    "        \"step_size\": 150000,\n",
    "        # Do not save, only train now\n",
    "        \"save_freq\": 400000,\n",
    "        # Warmup training \n",
    "        \"warmup\": True,\n",
    "        \"save_prefix\": \"/media/data/depth_models/vnet2_nyu_official_l2_only\"\n",
    "    })\n",
    "\n",
    "    p = Pipeline(config={\n",
    "        \"validation_frequency\": 50,\n",
    "        \"cycles\": 46000\n",
    "    })\n",
    "    p.add(db_loader)\n",
    "    p.add(transformer)\n",
    "    p.add(optimizer)\n",
    "    p.run()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Load parameters & compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = build_u_graph()\n",
    "g.load_weights(\"/media/data/depth_models/vnet2_logloss_high_momentum_iter_209938.zip\")\n",
    "g.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer on training or validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py, numpy as np\n",
    "f = h5py.File(\"/media/data/depth_data/nyu_depth_combined_vnet2.hdf5\")\n",
    "b = int(f[\"images\"].shape[0] * 0.9)\n",
    "images = np.array(f[\"images\"][b:])\n",
    "depths = np.array(f[\"depths\"][b:])\n",
    "print images.shape\n",
    "mean = np.load(\"/media/data/depth_data/nyu_depth_combined_vnet2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from deepgraph.nn.core import Dropout\n",
    "import time\n",
    "\n",
    "plot = True\n",
    "idx = 0\n",
    "diffs = []\n",
    "timings = []\n",
    "Dropout.set_dp_off()\n",
    "for image in images:\n",
    "    tmp = image.astype(np.float32).copy()\n",
    "    # Stochastic normalization\n",
    "    tmp -= mean\n",
    "    tmp *= 1.0 / 72.240064849107824\n",
    "    # center crop\n",
    "    cy = (tmp.shape[1] - 240 ) // 2\n",
    "    cx = (tmp.shape[2] - 320 ) // 2\n",
    "    tmp = tmp[:,cy:cy+240,cx:cx+320]\n",
    "    # Infer and measure time\n",
    "    start = time.time()\n",
    "    res = g.infer([tmp.reshape((1,3,240,320))])[\"conv_23_with_bias\"]\n",
    "    end = time.time()\n",
    "    timings.append(end - start)\n",
    "    res = res.squeeze()\n",
    "    res = np.exp(res)\n",
    "    depth = depths[idx]\n",
    "    tdepth = depth[cy:cy+240,cx:cx+320]\n",
    "\n",
    "    if plot and idx % 20 == 0:\n",
    "        \n",
    "        plt.imshow(image.transpose((1,2,0)).astype(np.uint8))\n",
    "        plt.show()\n",
    "        plt.imshow(tdepth)\n",
    "        plt.show()\n",
    "        plt.imshow(res)\n",
    "        plt.show()\n",
    "        print \"RMSE: \" + str(np.sqrt(np.mean((res-tdepth)**2)))\n",
    "        \n",
    "    diffs.append(res - tdepth)\n",
    "    \n",
    "    idx += 1\n",
    "    \n",
    "diffs = np.array(diffs)\n",
    "timings = np.array(timings)\n",
    "rmse = np.sqrt(np.mean(diffs ** 2))\n",
    "print \"Accumulated RMSE: \" + str(rmse)\n",
    "print \"Mean forward execution time (with Host -> GPU transfer): %f\" % timings.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer random images from the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib, cStringIO\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from deepgraph.nn.core import Dropout\n",
    "\n",
    "\n",
    "Dropout.set_dp_off()\n",
    "\n",
    "\n",
    "urls = [\n",
    "    'http://www.hotel-im-wasserturm.de/fileadmin/_processed_/csm_Deluxe_Doppelzimmer_2_6455fa52be.jpg',\n",
    "    'http://hotel-airport-zuerich.dorint.com/fileadmin/_processed_/csm_ZRH_Zimmer_DZ_04_bc3a12f87e.jpg',\n",
    "    'http://www.erlebnis-erlensee.de/images/bilder/partyservice/schnitzel.jpg',\n",
    "    'http://www.hankewitz.com/wp-content/uploads/2012/12/bolognese21.jpg'\n",
    "       ]\n",
    "idx = 0\n",
    "for URL in urls:\n",
    "    file = cStringIO.StringIO(urllib.urlopen(URL).read())\n",
    "    img = Image.open(file)\n",
    "    i_s = img.resize((344,258))\n",
    "    rs = np.array(i_s).transpose((2,0,1)).astype(np.float32)\n",
    "\n",
    "    #mean = np.load(\"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.npy\")\n",
    "    \n",
    "    rs -= mean\n",
    "    rs *= 1.0 / 72.240064849107824\n",
    "    # center crop\n",
    "    cy = (rs.shape[1] - 240 ) // 2\n",
    "    cx = (rs.shape[2] - 320 ) // 2\n",
    "    rs = rs[:,cy:cy+240,cx:cx+320]\n",
    "    res = g.infer([rs.reshape((1,3,240,320))])[\"conv_23_with_bias\"]\n",
    "    res = np.exp(res)\n",
    "    \n",
    "    plt.imshow(i_s)\n",
    "    plt.show()\n",
    "    plt.imshow(res.squeeze())\n",
    "    plt.show()\n",
    "    \n",
    "    print res.max()\n",
    "    # Store\n",
    "    np.save(\"url_image_%i.npy\" % idx, res)\n",
    "    idx += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer image from food data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib, cStringIO\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from deepgraph.nn.core import Dropout\n",
    "import numpy as np\n",
    "\n",
    "Dropout.set_dp_off()\n",
    "\n",
    "data = np.load('/home/ga29mix/nashome/data/kinectv2_2016_03_30_13_33_25_bgr.npy')[:,:,0:3]\n",
    "dd = np.load('/home/ga29mix/nashome/data/kinectv2_2016_03_30_13_33_25_depth.npy')\n",
    "\n",
    "img = Image.fromarray(np.array(data[:,:,::-1]))\n",
    "rs = img.resize((320,240))\n",
    "mean = np.load(\"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.npy\")\n",
    "arr = np.array(rs).transpose((2,0,1)).astype(np.float32)\n",
    "arr -= mean\n",
    "arr *= 1.0 / 72.240064849107824\n",
    "res = g.infer([arr.reshape((1,3,240,320))])[\"conv_23_with_bias\"]\n",
    "res = np.exp(res)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.imshow(res.squeeze())\n",
    "plt.show()\n",
    "plt.imshow(dd)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
