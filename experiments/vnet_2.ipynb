{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 90.0% of memory, CuDNN 3007)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  _____                _____                 _\n",
      " |  _  \\              |  __ \\               | |\n",
      " | | | |___  ___ _ __ | |  \\/_ __ __ _ _ __ | |__\n",
      " | | | / _ \\/ _ \\ '_ \\| | __| '__/ _` | '_ \\| '_ \\\n",
      " | |/ /  __/  __/ |_) | |_\\ \\ | | (_| | |_) | | | |\n",
      " |___/ \\___|\\___| .__/ \\____/_|  \\__,_| .__/|_| |_|\n",
      "                | |                   | |\n",
      "                |_|                   |_|\n",
      "\n",
      "\n",
      "Available on GitHub: https://github.com/sebastian-schlecht/deepgraph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from deepgraph.utils.logging import log\n",
    "from deepgraph.utils.common import batch_parallel, ConfigMixin, shuffle_in_unison_inplace, pickle_dump\n",
    "from deepgraph.utils.image import batch_pad_mirror\n",
    "from deepgraph.constants import *\n",
    "from deepgraph.conf import rng\n",
    "\n",
    "from deepgraph.pipeline import Processor, Packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepgraph.nn.init import *\n",
    "class Transformer(Processor):\n",
    "    \"\"\"\n",
    "    Apply online random augmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, shapes, config, buffer_size=10):\n",
    "        super(Transformer, self).__init__(name, shapes, config, buffer_size)\n",
    "        self.mean = None\n",
    "\n",
    "    def init(self):\n",
    "        if self.conf(\"mean_file\") is not None:\n",
    "            self.mean = np.load(self.conf(\"mean_file\"))\n",
    "        else:\n",
    "            log(\"Transformer - No mean file specified.\", LOG_LEVEL_WARNING)\n",
    "\n",
    "    def process(self):\n",
    "        packet = self.pull()\n",
    "        # Return if no data is there\n",
    "        if not packet:\n",
    "            return False\n",
    "        # Unpack\n",
    "        data, label = packet.data\n",
    "        # Do processing\n",
    "        log(\"Transformer - Processing data\", LOG_LEVEL_VERBOSE)\n",
    "        \n",
    "        h = 240\n",
    "        w = 320\n",
    "        \n",
    "        start = time.time()\n",
    "        # Mean\n",
    "        if packet.phase == PHASE_TRAIN or packet.phase == PHASE_VAL:\n",
    "            data = data.astype(np.float32)\n",
    "            if self.mean is not None:\n",
    "                std = self.conf(\"std\")\n",
    "                for idx in range(data.shape[0]):\n",
    "                    # Subtract mean\n",
    "                    data[idx] = data[idx] - self.mean.astype(np.float32)\n",
    "                    if std is not None:\n",
    "                        data[idx] =  data[idx] * std\n",
    "            if self.conf(\"offset\") is not None:\n",
    "                label -= self.conf(\"offset\")\n",
    "\n",
    "        if packet.phase == PHASE_TRAIN:\n",
    "             # Do elementwise operations\n",
    "            data_old = data\n",
    "            label_old = label\n",
    "            data = np.zeros((data_old.shape[0], data_old.shape[1], h, w), dtype=np.float32)\n",
    "            label = np.zeros((label_old.shape[0], h, w), dtype=np.float32)\n",
    "            for idx in range(data.shape[0]):\n",
    "                # Rotate\n",
    "                # We rotate before cropping to be able to get filled corners\n",
    "                # Maybe even adjust the border after rotating\n",
    "                deg = np.random.randint(-5,6)\n",
    "                # Operate on old data. Careful - data is already in float so we need to normalize and rescale afterwards\n",
    "                # data_old[idx] = 255. * rotate_transformer_rgb_uint8(data_old[idx] * 0.003921568627, deg).astype(np.float32)\n",
    "                # label_old[idx] = rotate_transformer_scalar_float32(label_old[idx], deg)\n",
    "                \n",
    "                # Take care of any empty areas, we crop on a smaller surface depending on the angle\n",
    "                # TODO Remove this once loss supports masking\n",
    "                shift = 0 #np.tan((deg/180.) * math.pi)\n",
    "                # Random crops\n",
    "                #cy = rng.randint(data_old.shape[2] - h - shift, size=1)\n",
    "                #cx = rng.randint(data_old.shape[3] - w - shift, size=1)\n",
    "\n",
    "                data[idx] = data_old[idx]\n",
    "                label[idx] = label_old[idx]\n",
    "\n",
    "                # Flip horizontally with probability 0.5\n",
    "                \"\"\"\n",
    "                p = rng.randint(2)\n",
    "                if p > 0:\n",
    "                    data[idx] = data[idx, :, :, ::-1]\n",
    "                    label[idx] = label[idx, :, ::-1]\n",
    "\n",
    "                # RGB we mult with a random value between 0.8 and 1.2\n",
    "                r = rng.randint(80,121) / 100.\n",
    "                g = rng.randint(80,121) / 100.\n",
    "                b = rng.randint(80,121) / 100.\n",
    "                data[idx, 0] = data[idx, 0] * r\n",
    "                data[idx, 1] = data[idx, 1] * g\n",
    "                data[idx, 2] = data[idx, 2] * b\n",
    "                \"\"\"\n",
    "            # Shuffle\n",
    "            # data, label = shuffle_in_unison_inplace(data, label)\n",
    "            \n",
    "        elif packet.phase == PHASE_VAL:\n",
    "            # Center crop\n",
    "            pass\n",
    "            #cy = (data.shape[2] - h) // 2\n",
    "            #cx = (data.shape[3] - w) // 2\n",
    "            #data = data[:, :, cy:cy+h, cx:cx+w]\n",
    "            #label = label[:, cy:cy+h, cx:cx+w]\n",
    "            \n",
    "        end = time.time()\n",
    "        log(\"Transformer - Processing took \" + str(end - start) + \" seconds.\", LOG_LEVEL_VERBOSE)\n",
    "        # Try to push into queue as long as thread should not terminate\n",
    "        self.push(Packet(identifier=packet.id, phase=packet.phase, num=2, data=(data, label)))\n",
    "        return True\n",
    "\n",
    "    def setup_defaults(self):\n",
    "        super(Transformer, self).setup_defaults()\n",
    "        self.conf_default(\"mean_file\", None)\n",
    "        self.conf_default(\"offset\", None)\n",
    "        self.conf_default(\"std\", 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-04-15 12:06:06] INFO: H5DBLoader - Caching DB in memory\n",
      "[2016-04-15 12:06:57] INFO: Pipeline - Starting computation\n",
      "[2016-04-15 12:06:58] INFO: Graph - Setting up graph\n",
      "[2016-04-15 12:06:58] INFO: Node - data has shape (-1, 3, 240, 320)\n",
      "[2016-04-15 12:06:58] INFO: Node - label has shape (-1, 1, 240, 320)\n",
      "[2016-04-15 12:06:58] INFO: Node - conv_1 has shape (-1, 64, 240, 320)\n",
      "[2016-04-15 12:06:58] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:06:58] INFO: Node - conv_2 has shape (-1, 64, 240, 320)\n",
      "[2016-04-15 12:06:58] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:06:58] INFO: Node - pool_2 has shape (-1, 64, 120, 160)\n",
      "[2016-04-15 12:06:58] INFO: Pool - Using DNN CUDA Module\n",
      "[2016-04-15 12:06:58] INFO: Node - conv_3 has shape (-1, 128, 120, 160)\n",
      "[2016-04-15 12:06:58] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:06:58] INFO: Node - conv_4 has shape (-1, 128, 120, 160)\n",
      "[2016-04-15 12:06:58] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:06:58] INFO: Node - pool_4 has shape (-1, 128, 60, 80)\n",
      "[2016-04-15 12:06:58] INFO: Pool - Using DNN CUDA Module\n",
      "[2016-04-15 12:06:58] INFO: Node - conv_5 has shape (-1, 256, 60, 80)\n",
      "[2016-04-15 12:06:58] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:06:58] INFO: Node - conv_6 has shape (-1, 256, 60, 80)\n",
      "[2016-04-15 12:06:58] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:06:58] INFO: Node - pool_6 has shape (-1, 256, 30, 40)\n",
      "[2016-04-15 12:06:58] INFO: Pool - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:09] INFO: Node - conv_7 has shape (-1, 512, 30, 40)\n",
      "[2016-04-15 12:08:09] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:09] INFO: Node - conv_8 has shape (-1, 512, 30, 40)\n",
      "[2016-04-15 12:08:09] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:09] INFO: Node - pool_8 has shape (-1, 512, 15, 20)\n",
      "[2016-04-15 12:08:09] INFO: Pool - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:09] INFO: Node - fl has shape (-1, 153600)\n",
      "[2016-04-15 12:08:42] INFO: Node - fc_8 has shape (-1, 4096)\n",
      "[2016-04-15 12:08:42] INFO: Node - dp_8 has shape (-1, 4096)\n",
      "[2016-04-15 12:08:46] INFO: Node - fc_9 has shape (-1, 19200)\n",
      "[2016-04-15 12:08:46] INFO: Node - dp_9 has shape (-1, 19200)\n",
      "[2016-04-15 12:08:46] INFO: Node - rs_10 has shape (-1, 64, 15, 20)\n",
      "[2016-04-15 12:08:46] INFO: Node - up_11 has shape (-1, 64, 30, 40)\n",
      "[2016-04-15 12:08:46] INFO: Node - conv_11 has shape (-1, 512, 30, 40)\n",
      "[2016-04-15 12:08:46] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:46] INFO: Node - concat_11 has shape (-1, 1024, 30, 40)\n",
      "[2016-04-15 12:08:46] INFO: Node - conv_12 has shape (-1, 512, 30, 40)\n",
      "[2016-04-15 12:08:46] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_13 has shape (-1, 512, 30, 40)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - up_14 has shape (-1, 512, 60, 80)\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_14 has shape (-1, 256, 60, 80)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - concat_14 has shape (-1, 512, 60, 80)\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_15 has shape (-1, 256, 60, 80)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_16 has shape (-1, 256, 60, 80)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - up_17 has shape (-1, 256, 120, 160)\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_17 has shape (-1, 128, 120, 160)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - concat_17 has shape (-1, 256, 120, 160)\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_18 has shape (-1, 128, 120, 160)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_19 has shape (-1, 128, 120, 160)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - up_20 has shape (-1, 128, 240, 320)\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_20 has shape (-1, 64, 240, 320)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - concat_20 has shape (-1, 128, 240, 320)\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_21 has shape (-1, 64, 240, 320)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_22 has shape (-1, 64, 240, 320)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - conv_23 has shape (-1, 1, 240, 320)\n",
      "[2016-04-15 12:08:47] INFO: Conv2D - Using DNN CUDA Module\n",
      "[2016-04-15 12:08:47] INFO: Node - loss has shape (1,)\n",
      "[2016-04-15 12:08:47] INFO: Node - mse has shape (1,)\n",
      "[2016-04-15 12:09:06] INFO: Graph - Invoking Theano compiler\n",
      "[2016-04-15 12:09:44] INFO: Optimizer - Compilation finished\n",
      "[2016-04-15 12:10:34] INFO: Optimizer - Training score at iteration 50: {'loss': array(3.6898860931396484, dtype=float32), 'mse': array(1.9209076166152954, dtype=float32)}\n",
      "[2016-04-15 12:11:26] INFO: Optimizer - Training score at iteration 100: {'loss': array(6.039698600769043, dtype=float32), 'mse': array(2.4575798511505127, dtype=float32)}\n",
      "[2016-04-15 12:12:18] INFO: Optimizer - Training score at iteration 150: {'loss': array(4.121854305267334, dtype=float32), 'mse': array(2.0302350521087646, dtype=float32)}\n",
      "[2016-04-15 12:13:10] INFO: Optimizer - Training score at iteration 200: {'loss': array(1.2761114835739136, dtype=float32), 'mse': array(1.1296510696411133, dtype=float32)}\n",
      "[2016-04-15 12:13:31] INFO: Optimizer - Mean loss values for validation at iteration 200 is: {'loss': 3.9892375, 'mse': 1.9299775}\n",
      "[2016-04-15 12:14:22] INFO: Optimizer - Training score at iteration 250: {'loss': array(1.8368662595748901, dtype=float32), 'mse': array(1.3553104400634766, dtype=float32)}\n",
      "[2016-04-15 12:15:13] INFO: Optimizer - Training score at iteration 300: {'loss': array(3.3300416469573975, dtype=float32), 'mse': array(1.8248401880264282, dtype=float32)}\n",
      "[2016-04-15 12:16:04] INFO: Optimizer - Training score at iteration 350: {'loss': array(2.932969093322754, dtype=float32), 'mse': array(1.712591290473938, dtype=float32)}\n",
      "[2016-04-15 12:16:56] INFO: Optimizer - Training score at iteration 400: {'loss': array(5.348709583282471, dtype=float32), 'mse': array(2.312727689743042, dtype=float32)}\n",
      "[2016-04-15 12:17:17] INFO: Optimizer - Mean loss values for validation at iteration 400 is: {'loss': 3.4877591, 'mse': 1.8196418}\n",
      "[2016-04-15 12:18:08] INFO: Optimizer - Training score at iteration 450: {'loss': array(6.388795375823975, dtype=float32), 'mse': array(2.52760648727417, dtype=float32)}\n",
      "[2016-04-15 12:18:59] INFO: Optimizer - Training score at iteration 500: {'loss': array(1.601336121559143, dtype=float32), 'mse': array(1.2654390335083008, dtype=float32)}\n",
      "[2016-04-15 12:19:50] INFO: Optimizer - Training score at iteration 550: {'loss': array(2.7529220581054688, dtype=float32), 'mse': array(1.6591931581497192, dtype=float32)}\n",
      "[2016-04-15 12:20:41] INFO: Optimizer - Training score at iteration 600: {'loss': array(3.1141138076782227, dtype=float32), 'mse': array(1.7646851539611816, dtype=float32)}\n",
      "[2016-04-15 12:21:03] INFO: Optimizer - Mean loss values for validation at iteration 600 is: {'loss': 3.2145681, 'mse': 1.7353328}\n",
      "[2016-04-15 12:21:54] INFO: Optimizer - Training score at iteration 650: {'loss': array(1.4677759408950806, dtype=float32), 'mse': array(1.2115180492401123, dtype=float32)}\n",
      "[2016-04-15 12:22:45] INFO: Optimizer - Training score at iteration 700: {'loss': array(2.00187611579895, dtype=float32), 'mse': array(1.4148766994476318, dtype=float32)}\n",
      "[2016-04-15 12:23:37] INFO: Optimizer - Training score at iteration 750: {'loss': array(3.4963295459747314, dtype=float32), 'mse': array(1.8698474168777466, dtype=float32)}\n",
      "[2016-04-15 12:24:28] INFO: Optimizer - Training score at iteration 800: {'loss': array(2.919304609298706, dtype=float32), 'mse': array(1.708597183227539, dtype=float32)}\n",
      "[2016-04-15 12:24:50] INFO: Optimizer - Mean loss values for validation at iteration 800 is: {'loss': 3.1505487, 'mse': 1.7089934}\n",
      "[2016-04-15 12:25:41] INFO: Optimizer - Training score at iteration 850: {'loss': array(6.182797908782959, dtype=float32), 'mse': array(2.486523151397705, dtype=float32)}\n",
      "[2016-04-15 12:26:32] INFO: Optimizer - Training score at iteration 900: {'loss': array(2.407020330429077, dtype=float32), 'mse': array(1.551457405090332, dtype=float32)}\n",
      "[2016-04-15 12:27:24] INFO: Optimizer - Training score at iteration 950: {'loss': array(0.6334241032600403, dtype=float32), 'mse': array(0.7958794236183167, dtype=float32)}\n",
      "[2016-04-15 12:28:36] INFO: Optimizer - Mean loss values for validation at iteration 999 is: {'loss': 3.0468802, 'mse': 1.6804588}\n",
      "[2016-04-15 12:28:36] INFO: Optimizer - Training score at iteration 1000: {'loss': array(0.9695623517036438, dtype=float32), 'mse': array(0.9846635460853577, dtype=float32)}\n",
      "[2016-04-15 12:29:28] INFO: Optimizer - Training score at iteration 1050: {'loss': array(2.4216063022613525, dtype=float32), 'mse': array(1.5561511516571045, dtype=float32)}\n",
      "[2016-04-15 12:30:19] INFO: Optimizer - Training score at iteration 1100: {'loss': array(1.0969470739364624, dtype=float32), 'mse': array(1.0473524332046509, dtype=float32)}\n",
      "[2016-04-15 12:31:10] INFO: Optimizer - Training score at iteration 1150: {'loss': array(3.954561233520508, dtype=float32), 'mse': array(1.9886078834533691, dtype=float32)}\n",
      "[2016-04-15 12:32:22] INFO: Optimizer - Mean loss values for validation at iteration 1199 is: {'loss': 2.9987833, 'mse': 1.6660901}\n",
      "[2016-04-15 12:32:23] INFO: Optimizer - Training score at iteration 1200: {'loss': array(1.3546457290649414, dtype=float32), 'mse': array(1.1638925075531006, dtype=float32)}\n",
      "[2016-04-15 12:33:14] INFO: Optimizer - Training score at iteration 1250: {'loss': array(1.1276434659957886, dtype=float32), 'mse': array(1.0619055032730103, dtype=float32)}\n",
      "[2016-04-15 12:34:06] INFO: Optimizer - Training score at iteration 1300: {'loss': array(1.1437352895736694, dtype=float32), 'mse': array(1.069455623626709, dtype=float32)}\n",
      "[2016-04-15 12:34:57] INFO: Optimizer - Training score at iteration 1350: {'loss': array(0.9916593432426453, dtype=float32), 'mse': array(0.995820939540863, dtype=float32)}\n",
      "[2016-04-15 12:36:09] INFO: Optimizer - Mean loss values for validation at iteration 1399 is: {'loss': 2.9808652, 'mse': 1.6721261}\n",
      "[2016-04-15 12:36:10] INFO: Optimizer - Training score at iteration 1400: {'loss': array(5.2344818115234375, dtype=float32), 'mse': array(2.2878990173339844, dtype=float32)}\n",
      "[2016-04-15 12:37:01] INFO: Optimizer - Training score at iteration 1450: {'loss': array(4.200952529907227, dtype=float32), 'mse': array(2.0496225357055664, dtype=float32)}\n",
      "[2016-04-15 12:37:53] INFO: Optimizer - Training score at iteration 1500: {'loss': array(1.632554054260254, dtype=float32), 'mse': array(1.2777143716812134, dtype=float32)}\n",
      "[2016-04-15 12:38:44] INFO: Optimizer - Training score at iteration 1550: {'loss': array(1.9021379947662354, dtype=float32), 'mse': array(1.3791801929473877, dtype=float32)}\n",
      "[2016-04-15 12:39:56] INFO: Optimizer - Mean loss values for validation at iteration 1599 is: {'loss': 3.0878115, 'mse': 1.7135918}\n",
      "[2016-04-15 12:39:57] INFO: Optimizer - Training score at iteration 1600: {'loss': array(1.6363474130630493, dtype=float32), 'mse': array(1.2791979312896729, dtype=float32)}\n",
      "[2016-04-15 12:40:48] INFO: Optimizer - Training score at iteration 1650: {'loss': array(5.3735575675964355, dtype=float32), 'mse': array(2.3180935382843018, dtype=float32)}\n",
      "[2016-04-15 12:41:40] INFO: Optimizer - Training score at iteration 1700: {'loss': array(3.662386417388916, dtype=float32), 'mse': array(1.9137362241744995, dtype=float32)}\n",
      "[2016-04-15 12:42:31] INFO: Optimizer - Training score at iteration 1750: {'loss': array(0.9008821845054626, dtype=float32), 'mse': array(0.9491481184959412, dtype=float32)}\n",
      "[2016-04-15 12:43:42] INFO: Optimizer - Mean loss values for validation at iteration 1798 is: {'loss': 2.9346809, 'mse': 1.6455784}\n",
      "[2016-04-15 12:43:44] INFO: Optimizer - Training score at iteration 1800: {'loss': array(0.7771921753883362, dtype=float32), 'mse': array(0.8815850019454956, dtype=float32)}\n",
      "[2016-04-15 12:44:35] INFO: Optimizer - Training score at iteration 1850: {'loss': array(3.7727890014648438, dtype=float32), 'mse': array(1.9423668384552002, dtype=float32)}\n",
      "[2016-04-15 12:45:27] INFO: Optimizer - Training score at iteration 1900: {'loss': array(1.1306822299957275, dtype=float32), 'mse': array(1.0633354187011719, dtype=float32)}\n",
      "[2016-04-15 12:46:18] INFO: Optimizer - Training score at iteration 1950: {'loss': array(4.445053577423096, dtype=float32), 'mse': array(2.1083295345306396, dtype=float32)}\n",
      "[2016-04-15 12:47:29] INFO: Optimizer - Mean loss values for validation at iteration 1998 is: {'loss': 2.8823514, 'mse': 1.6310947}\n",
      "[2016-04-15 12:47:31] INFO: Optimizer - Training score at iteration 2000: {'loss': array(1.00186288356781, dtype=float32), 'mse': array(1.0009310245513916, dtype=float32)}\n",
      "[2016-04-15 12:48:22] INFO: Optimizer - Training score at iteration 2050: {'loss': array(3.3791234493255615, dtype=float32), 'mse': array(1.8382391929626465, dtype=float32)}\n",
      "[2016-04-15 12:49:14] INFO: Optimizer - Training score at iteration 2100: {'loss': array(0.8788202404975891, dtype=float32), 'mse': array(0.937454104423523, dtype=float32)}\n",
      "[2016-04-15 12:50:05] INFO: Optimizer - Training score at iteration 2150: {'loss': array(5.990590572357178, dtype=float32), 'mse': array(2.44756817817688, dtype=float32)}\n",
      "[2016-04-15 12:51:16] INFO: Optimizer - Mean loss values for validation at iteration 2198 is: {'loss': 2.8677907, 'mse': 1.6253463}\n",
      "[2016-04-15 12:51:18] INFO: Optimizer - Training score at iteration 2200: {'loss': array(4.118265151977539, dtype=float32), 'mse': array(2.029350996017456, dtype=float32)}\n",
      "[2016-04-15 12:52:09] INFO: Optimizer - Training score at iteration 2250: {'loss': array(4.3525214195251465, dtype=float32), 'mse': array(2.0862696170806885, dtype=float32)}\n",
      "[2016-04-15 12:53:01] INFO: Optimizer - Training score at iteration 2300: {'loss': array(5.397786617279053, dtype=float32), 'mse': array(2.3233137130737305, dtype=float32)}\n",
      "[2016-04-15 12:53:52] INFO: Optimizer - Training score at iteration 2350: {'loss': array(0.8001124262809753, dtype=float32), 'mse': array(0.8944900035858154, dtype=float32)}\n",
      "[2016-04-15 12:55:03] INFO: Optimizer - Mean loss values for validation at iteration 2398 is: {'loss': 2.8701539, 'mse': 1.6406822}\n",
      "[2016-04-15 12:55:05] INFO: Optimizer - Training score at iteration 2400: {'loss': array(2.208056926727295, dtype=float32), 'mse': array(1.4859532117843628, dtype=float32)}\n",
      "[2016-04-15 12:55:57] INFO: Optimizer - Training score at iteration 2450: {'loss': array(2.1976077556610107, dtype=float32), 'mse': array(1.4824330806732178, dtype=float32)}\n",
      "[2016-04-15 12:56:48] INFO: Optimizer - Training score at iteration 2500: {'loss': array(0.7919110655784607, dtype=float32), 'mse': array(0.8898938298225403, dtype=float32)}\n",
      "[2016-04-15 12:57:40] INFO: Optimizer - Training score at iteration 2550: {'loss': array(1.6744413375854492, dtype=float32), 'mse': array(1.2940020561218262, dtype=float32)}\n",
      "[2016-04-15 12:58:50] INFO: Optimizer - Mean loss values for validation at iteration 2597 is: {'loss': 2.8931336, 'mse': 1.6250683}\n",
      "[2016-04-15 12:58:53] INFO: Optimizer - Training score at iteration 2600: {'loss': array(1.6555432081222534, dtype=float32), 'mse': array(1.2866791486740112, dtype=float32)}\n",
      "[2016-04-15 12:59:44] INFO: Optimizer - Training score at iteration 2650: {'loss': array(3.1531875133514404, dtype=float32), 'mse': array(1.7757216691970825, dtype=float32)}\n",
      "[2016-04-15 13:00:36] INFO: Optimizer - Training score at iteration 2700: {'loss': array(2.457340955734253, dtype=float32), 'mse': array(1.5675908327102661, dtype=float32)}\n",
      "[2016-04-15 13:01:27] INFO: Optimizer - Training score at iteration 2750: {'loss': array(2.7218410968780518, dtype=float32), 'mse': array(1.6498003005981445, dtype=float32)}\n",
      "[2016-04-15 13:02:37] INFO: Optimizer - Mean loss values for validation at iteration 2797 is: {'loss': 2.8316016, 'mse': 1.6126974}\n",
      "[2016-04-15 13:02:40] INFO: Optimizer - Training score at iteration 2800: {'loss': array(3.6440606117248535, dtype=float32), 'mse': array(1.9089422225952148, dtype=float32)}\n",
      "[2016-04-15 13:03:31] INFO: Optimizer - Training score at iteration 2850: {'loss': array(1.7267907857894897, dtype=float32), 'mse': array(1.3140740394592285, dtype=float32)}\n",
      "[2016-04-15 13:04:22] INFO: Optimizer - Training score at iteration 2900: {'loss': array(4.331826686859131, dtype=float32), 'mse': array(2.0813040733337402, dtype=float32)}\n",
      "[2016-04-15 13:05:14] INFO: Optimizer - Training score at iteration 2950: {'loss': array(1.3107174634933472, dtype=float32), 'mse': array(1.1448657512664795, dtype=float32)}\n",
      "[2016-04-15 13:06:24] INFO: Optimizer - Mean loss values for validation at iteration 2997 is: {'loss': 2.796242, 'mse': 1.6110734}\n",
      "[2016-04-15 13:06:27] INFO: Optimizer - Training score at iteration 3000: {'loss': array(1.2418111562728882, dtype=float32), 'mse': array(1.114365816116333, dtype=float32)}\n",
      "[2016-04-15 13:06:27] INFO: Optimizer - Saving intermediate model state\n",
      "[2016-04-15 13:08:44] INFO: Graph - Model file saved as: ../data/vnet2_iter_3000.zip\n",
      "[2016-04-15 13:09:43] INFO: Optimizer - Training score at iteration 3050: {'loss': array(1.6866401433944702, dtype=float32), 'mse': array(1.298707127571106, dtype=float32)}\n",
      "[2016-04-15 13:10:36] INFO: Optimizer - Training score at iteration 3100: {'loss': array(5.160858631134033, dtype=float32), 'mse': array(2.27175235748291, dtype=float32)}\n",
      "[2016-04-15 13:11:29] INFO: Optimizer - Training score at iteration 3150: {'loss': array(3.984471082687378, dtype=float32), 'mse': array(1.9961140155792236, dtype=float32)}\n",
      "[2016-04-15 13:12:41] INFO: Optimizer - Mean loss values for validation at iteration 3197 is: {'loss': 2.7955372, 'mse': 1.6031873}\n",
      "[2016-04-15 13:12:43] INFO: Optimizer - Training score at iteration 3200: {'loss': array(1.746590495109558, dtype=float32), 'mse': array(1.3215863704681396, dtype=float32)}\n",
      "[2016-04-15 13:13:35] INFO: Optimizer - Training score at iteration 3250: {'loss': array(3.478565216064453, dtype=float32), 'mse': array(1.8650912046432495, dtype=float32)}\n",
      "[2016-04-15 13:14:27] INFO: Optimizer - Training score at iteration 3300: {'loss': array(1.7409676313400269, dtype=float32), 'mse': array(1.3194572925567627, dtype=float32)}\n",
      "[2016-04-15 13:15:19] INFO: Optimizer - Training score at iteration 3350: {'loss': array(1.0467818975448608, dtype=float32), 'mse': array(1.0231236219406128, dtype=float32)}\n",
      "[2016-04-15 13:16:28] INFO: Optimizer - Mean loss values for validation at iteration 3396 is: {'loss': 2.7949743, 'mse': 1.6020783}\n",
      "[2016-04-15 13:16:32] INFO: Optimizer - Training score at iteration 3400: {'loss': array(2.875065565109253, dtype=float32), 'mse': array(1.6956018209457397, dtype=float32)}\n",
      "[2016-04-15 13:17:24] INFO: Optimizer - Training score at iteration 3450: {'loss': array(5.846728324890137, dtype=float32), 'mse': array(2.4180009365081787, dtype=float32)}\n",
      "[2016-04-15 13:18:15] INFO: Optimizer - Training score at iteration 3500: {'loss': array(2.1856343746185303, dtype=float32), 'mse': array(1.4783891439437866, dtype=float32)}\n",
      "[2016-04-15 13:19:06] INFO: Optimizer - Training score at iteration 3550: {'loss': array(1.048744559288025, dtype=float32), 'mse': array(1.0240823030471802, dtype=float32)}\n",
      "[2016-04-15 13:20:15] INFO: Optimizer - Mean loss values for validation at iteration 3596 is: {'loss': 2.7825544, 'mse': 1.6129748}\n",
      "[2016-04-15 13:20:19] INFO: Optimizer - Training score at iteration 3600: {'loss': array(2.461890459060669, dtype=float32), 'mse': array(1.5690412521362305, dtype=float32)}\n",
      "[2016-04-15 13:21:10] INFO: Optimizer - Training score at iteration 3650: {'loss': array(2.9446334838867188, dtype=float32), 'mse': array(1.7159934043884277, dtype=float32)}\n",
      "[2016-04-15 13:22:02] INFO: Optimizer - Training score at iteration 3700: {'loss': array(1.667388916015625, dtype=float32), 'mse': array(1.2912741899490356, dtype=float32)}\n",
      "[2016-04-15 13:22:54] INFO: Optimizer - Training score at iteration 3750: {'loss': array(2.0587375164031982, dtype=float32), 'mse': array(1.4348301887512207, dtype=float32)}\n",
      "[2016-04-15 13:24:03] INFO: Optimizer - Mean loss values for validation at iteration 3796 is: {'loss': 2.7541695, 'mse': 1.6021954}\n",
      "[2016-04-15 13:24:06] INFO: Optimizer - Training score at iteration 3800: {'loss': array(0.6491919755935669, dtype=float32), 'mse': array(0.8057245016098022, dtype=float32)}\n",
      "[2016-04-15 13:24:58] INFO: Optimizer - Training score at iteration 3850: {'loss': array(2.189539909362793, dtype=float32), 'mse': array(1.4797093868255615, dtype=float32)}\n",
      "[2016-04-15 13:25:49] INFO: Optimizer - Training score at iteration 3900: {'loss': array(4.219930171966553, dtype=float32), 'mse': array(2.0542469024658203, dtype=float32)}\n",
      "[2016-04-15 13:26:41] INFO: Optimizer - Training score at iteration 3950: {'loss': array(1.0025628805160522, dtype=float32), 'mse': array(1.0012805461883545, dtype=float32)}\n",
      "[2016-04-15 13:27:50] INFO: Optimizer - Mean loss values for validation at iteration 3996 is: {'loss': 2.7558253, 'mse': 1.6055255}\n",
      "[2016-04-15 13:27:54] INFO: Optimizer - Training score at iteration 4000: {'loss': array(4.58544397354126, dtype=float32), 'mse': array(2.1413650512695312, dtype=float32)}\n",
      "[2016-04-15 13:28:45] INFO: Optimizer - Training score at iteration 4050: {'loss': array(1.764044165611267, dtype=float32), 'mse': array(1.328173279762268, dtype=float32)}\n",
      "[2016-04-15 13:29:36] INFO: Optimizer - Training score at iteration 4100: {'loss': array(0.8692458271980286, dtype=float32), 'mse': array(0.9323335289955139, dtype=float32)}\n",
      "[2016-04-15 13:30:28] INFO: Optimizer - Training score at iteration 4150: {'loss': array(1.6209144592285156, dtype=float32), 'mse': array(1.2731513977050781, dtype=float32)}\n",
      "[2016-04-15 13:31:36] INFO: Optimizer - Mean loss values for validation at iteration 4195 is: {'loss': 2.7220306, 'mse': 1.5880781}\n",
      "[2016-04-15 13:31:41] INFO: Optimizer - Training score at iteration 4200: {'loss': array(1.2763537168502808, dtype=float32), 'mse': array(1.1297582387924194, dtype=float32)}\n",
      "[2016-04-15 13:32:32] INFO: Optimizer - Training score at iteration 4250: {'loss': array(4.529118061065674, dtype=float32), 'mse': array(2.1281723976135254, dtype=float32)}\n",
      "[2016-04-15 13:33:24] INFO: Optimizer - Training score at iteration 4300: {'loss': array(0.9983643889427185, dtype=float32), 'mse': array(0.999181866645813, dtype=float32)}\n",
      "[2016-04-15 13:34:15] INFO: Optimizer - Training score at iteration 4350: {'loss': array(2.4426980018615723, dtype=float32), 'mse': array(1.5629132986068726, dtype=float32)}\n",
      "[2016-04-15 13:35:23] INFO: Optimizer - Mean loss values for validation at iteration 4395 is: {'loss': 2.7170451, 'mse': 1.5902046}\n",
      "[2016-04-15 13:35:28] INFO: Optimizer - Training score at iteration 4400: {'loss': array(7.313144683837891, dtype=float32), 'mse': array(2.704282522201538, dtype=float32)}\n",
      "[2016-04-15 13:36:19] INFO: Optimizer - Training score at iteration 4450: {'loss': array(5.544116020202637, dtype=float32), 'mse': array(2.3545947074890137, dtype=float32)}\n",
      "[2016-04-15 13:37:11] INFO: Optimizer - Training score at iteration 4500: {'loss': array(3.2680790424346924, dtype=float32), 'mse': array(1.8077828884124756, dtype=float32)}\n",
      "[2016-04-15 13:38:02] INFO: Optimizer - Training score at iteration 4550: {'loss': array(2.557941198348999, dtype=float32), 'mse': array(1.5993565320968628, dtype=float32)}\n",
      "[2016-04-15 13:39:10] INFO: Optimizer - Mean loss values for validation at iteration 4595 is: {'loss': 3.0098283, 'mse': 1.6986328}\n",
      "[2016-04-15 13:39:15] INFO: Optimizer - Training score at iteration 4600: {'loss': array(3.304504156112671, dtype=float32), 'mse': array(1.8178294897079468, dtype=float32)}\n",
      "[2016-04-15 13:40:07] INFO: Optimizer - Training score at iteration 4650: {'loss': array(2.374776601791382, dtype=float32), 'mse': array(1.541031002998352, dtype=float32)}\n",
      "[2016-04-15 13:40:58] INFO: Optimizer - Training score at iteration 4700: {'loss': array(1.2189159393310547, dtype=float32), 'mse': array(1.1040452718734741, dtype=float32)}\n",
      "[2016-04-15 13:41:50] INFO: Optimizer - Training score at iteration 4750: {'loss': array(2.023629903793335, dtype=float32), 'mse': array(1.4225435256958008, dtype=float32)}\n",
      "[2016-04-15 13:42:57] INFO: Optimizer - Mean loss values for validation at iteration 4795 is: {'loss': 2.7334402, 'mse': 1.6023519}\n",
      "[2016-04-15 13:43:02] INFO: Optimizer - Training score at iteration 4800: {'loss': array(1.8386577367782593, dtype=float32), 'mse': array(1.355971097946167, dtype=float32)}\n",
      "[2016-04-15 13:43:54] INFO: Optimizer - Training score at iteration 4850: {'loss': array(1.8738847970962524, dtype=float32), 'mse': array(1.3688991069793701, dtype=float32)}\n",
      "[2016-04-15 13:44:45] INFO: Optimizer - Training score at iteration 4900: {'loss': array(0.9289280772209167, dtype=float32), 'mse': array(0.9638091325759888, dtype=float32)}\n",
      "[2016-04-15 13:45:37] INFO: Optimizer - Training score at iteration 4950: {'loss': array(1.5488895177841187, dtype=float32), 'mse': array(1.2445439100265503, dtype=float32)}\n",
      "[2016-04-15 13:46:44] INFO: Optimizer - Mean loss values for validation at iteration 4995 is: {'loss': 2.6995893, 'mse': 1.5741982}\n",
      "[2016-04-15 13:46:49] INFO: Optimizer - Training score at iteration 5000: {'loss': array(3.9133529663085938, dtype=float32), 'mse': array(1.9782196283340454, dtype=float32)}\n",
      "[2016-04-15 13:47:41] INFO: Optimizer - Training score at iteration 5050: {'loss': array(1.178904414176941, dtype=float32), 'mse': array(1.0857735872268677, dtype=float32)}\n",
      "[2016-04-15 13:48:32] INFO: Optimizer - Training score at iteration 5100: {'loss': array(1.0133557319641113, dtype=float32), 'mse': array(1.0066556930541992, dtype=float32)}\n",
      "[2016-04-15 13:49:24] INFO: Optimizer - Training score at iteration 5150: {'loss': array(2.6047146320343018, dtype=float32), 'mse': array(1.61391282081604, dtype=float32)}\n",
      "[2016-04-15 13:50:30] INFO: Optimizer - Mean loss values for validation at iteration 5194 is: {'loss': 2.6568432, 'mse': 1.5697215}\n",
      "[2016-04-15 13:50:36] INFO: Optimizer - Training score at iteration 5200: {'loss': array(2.5717933177948, dtype=float32), 'mse': array(1.603681206703186, dtype=float32)}\n",
      "[2016-04-15 13:51:28] INFO: Optimizer - Training score at iteration 5250: {'loss': array(0.7891891002655029, dtype=float32), 'mse': array(0.8883631229400635, dtype=float32)}\n",
      "[2016-04-15 13:52:19] INFO: Optimizer - Training score at iteration 5300: {'loss': array(5.454504013061523, dtype=float32), 'mse': array(2.3354878425598145, dtype=float32)}\n",
      "[2016-04-15 13:53:11] INFO: Optimizer - Training score at iteration 5350: {'loss': array(2.292912006378174, dtype=float32), 'mse': array(1.5142364501953125, dtype=float32)}\n",
      "[2016-04-15 13:54:18] INFO: Optimizer - Mean loss values for validation at iteration 5394 is: {'loss': 2.6389468, 'mse': 1.5614171}\n",
      "[2016-04-15 13:54:23] INFO: Optimizer - Training score at iteration 5400: {'loss': array(1.7653192281723022, dtype=float32), 'mse': array(1.32865309715271, dtype=float32)}\n",
      "[2016-04-15 13:55:15] INFO: Optimizer - Training score at iteration 5450: {'loss': array(1.98757803440094, dtype=float32), 'mse': array(1.4098148345947266, dtype=float32)}\n",
      "[2016-04-15 13:56:06] INFO: Optimizer - Training score at iteration 5500: {'loss': array(4.998775482177734, dtype=float32), 'mse': array(2.2357943058013916, dtype=float32)}\n",
      "[2016-04-15 13:56:58] INFO: Optimizer - Training score at iteration 5550: {'loss': array(2.54329252243042, dtype=float32), 'mse': array(1.5947703123092651, dtype=float32)}\n",
      "[2016-04-15 13:58:05] INFO: Optimizer - Mean loss values for validation at iteration 5594 is: {'loss': 2.6411781, 'mse': 1.5715328}\n",
      "[2016-04-15 13:58:11] INFO: Optimizer - Training score at iteration 5600: {'loss': array(3.3825206756591797, dtype=float32), 'mse': array(1.839163064956665, dtype=float32)}\n",
      "[2016-04-15 13:59:02] INFO: Optimizer - Training score at iteration 5650: {'loss': array(2.185026168823242, dtype=float32), 'mse': array(1.478183388710022, dtype=float32)}\n",
      "[2016-04-15 13:59:54] INFO: Optimizer - Training score at iteration 5700: {'loss': array(1.9164535999298096, dtype=float32), 'mse': array(1.3843603134155273, dtype=float32)}\n",
      "[2016-04-15 14:00:45] INFO: Optimizer - Training score at iteration 5750: {'loss': array(1.4647948741912842, dtype=float32), 'mse': array(1.210287094116211, dtype=float32)}\n",
      "[2016-04-15 14:01:52] INFO: Optimizer - Mean loss values for validation at iteration 5794 is: {'loss': 2.6571846, 'mse': 1.5814924}\n",
      "[2016-04-15 14:01:58] INFO: Optimizer - Training score at iteration 5800: {'loss': array(1.757467269897461, dtype=float32), 'mse': array(1.3256950378417969, dtype=float32)}\n",
      "[2016-04-15 14:02:49] INFO: Optimizer - Training score at iteration 5850: {'loss': array(1.1162073612213135, dtype=float32), 'mse': array(1.0565071105957031, dtype=float32)}\n",
      "[2016-04-15 14:03:41] INFO: Optimizer - Training score at iteration 5900: {'loss': array(1.0208338499069214, dtype=float32), 'mse': array(1.010363221168518, dtype=float32)}\n",
      "[2016-04-15 14:04:33] INFO: Optimizer - Training score at iteration 5950: {'loss': array(2.052830219268799, dtype=float32), 'mse': array(1.4327701330184937, dtype=float32)}\n",
      "[2016-04-15 14:05:38] INFO: Optimizer - Mean loss values for validation at iteration 5993 is: {'loss': 2.6837602, 'mse': 1.5651491}\n",
      "[2016-04-15 14:05:45] INFO: Optimizer - Training score at iteration 6000: {'loss': array(2.9813320636749268, dtype=float32), 'mse': array(1.7266534566879272, dtype=float32)}\n",
      "[2016-04-15 14:05:45] INFO: Optimizer - Saving intermediate model state\n",
      "[2016-04-15 14:08:15] INFO: Graph - Model file saved as: ../data/vnet2_iter_6000.zip\n",
      "[2016-04-15 14:09:06] INFO: Optimizer - Training score at iteration 6050: {'loss': array(3.8653500080108643, dtype=float32), 'mse': array(1.966049313545227, dtype=float32)}\n",
      "[2016-04-15 14:09:58] INFO: Optimizer - Training score at iteration 6100: {'loss': array(2.1321327686309814, dtype=float32), 'mse': array(1.4601824283599854, dtype=float32)}\n",
      "[2016-04-15 14:10:51] INFO: Optimizer - Training score at iteration 6150: {'loss': array(0.9187813997268677, dtype=float32), 'mse': array(0.958530843257904, dtype=float32)}\n",
      "[2016-04-15 14:11:25] INFO: Pipeline - All commands have been dispatched\n",
      "[2016-04-15 14:11:57] INFO: Optimizer - Mean loss values for validation at iteration 6193 is: {'loss': 2.5589616, 'mse': 1.5380768}\n",
      "[2016-04-15 14:12:04] INFO: Optimizer - Training score at iteration 6200: {'loss': array(1.3876805305480957, dtype=float32), 'mse': array(1.1779985427856445, dtype=float32)}\n",
      "[2016-04-15 14:12:56] INFO: Optimizer - Training score at iteration 6250: {'loss': array(0.9649991989135742, dtype=float32), 'mse': array(0.9823437333106995, dtype=float32)}\n",
      "[2016-04-15 14:13:48] INFO: Optimizer - Training score at iteration 6300: {'loss': array(2.944833278656006, dtype=float32), 'mse': array(1.7160515785217285, dtype=float32)}\n",
      "[2016-04-15 14:14:39] INFO: Optimizer - Training score at iteration 6350: {'loss': array(2.6850812435150146, dtype=float32), 'mse': array(1.6386216878890991, dtype=float32)}\n",
      "[2016-04-15 14:15:45] INFO: Optimizer - Mean loss values for validation at iteration 6393 is: {'loss': 2.5654776, 'mse': 1.5353386}\n",
      "[2016-04-15 14:15:52] INFO: Optimizer - Training score at iteration 6400: {'loss': array(2.0865869522094727, dtype=float32), 'mse': array(1.444502353668213, dtype=float32)}\n",
      "[2016-04-15 14:16:43] INFO: Optimizer - Training score at iteration 6450: {'loss': array(0.6583791971206665, dtype=float32), 'mse': array(0.8114056587219238, dtype=float32)}\n",
      "[2016-04-15 14:17:27] INFO: Pipeline - Complete signal received.\n",
      "[2016-04-15 14:17:27] INFO: Pipeline - Stopping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from theano.tensor.nnet import relu\n",
    "\n",
    "from deepgraph.graph import *\n",
    "from deepgraph.nn.core import *\n",
    "from deepgraph.nn.conv import *\n",
    "from deepgraph.nn.loss import *\n",
    "from deepgraph.solver import *\n",
    "from deepgraph.nn.init import *\n",
    "\n",
    "from deepgraph.pipeline import Optimizer, H5DBLoader, Pipeline\n",
    "\n",
    "\n",
    "def build_u_graph():\n",
    "    graph = Graph(\"u_depth\")\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    \"\"\"\n",
    "    data = Data(graph, \"data\", T.ftensor4, shape=(-1, 3, 240, 320))\n",
    "    label = Data(graph, \"label\", T.ftensor3, shape=(-1, 1, 240, 320), config={\n",
    "        \"phase\": PHASE_TRAIN\n",
    "    })\n",
    "    \"\"\"\n",
    "    Contractive part\n",
    "    \"\"\"\n",
    "    conv_1 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_1\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_2 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_2\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    pool_2 = Pool(graph, \"pool_2\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_3 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_3\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_4 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_4\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    pool_4 = Pool(graph, \"pool_4\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "\n",
    "    conv_5 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_5\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_6 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_6\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    pool_6 = Pool(graph, \"pool_6\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "\n",
    "    conv_7 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_7\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_8 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_8\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    pool_8 = Pool(graph, \"pool_8\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    \"\"\"\n",
    "    conv_9 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_9\",\n",
    "        config={\n",
    "            \"channels\": 1024,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_10 = Conv2D(\n",
    "            graph,\n",
    "            \"conv_10\",\n",
    "            config={\n",
    "                \"channels\": 1024,\n",
    "                \"kernel\": (3, 3),\n",
    "                \"border_mode\": (1, 1),\n",
    "                \"activation\": relu,\n",
    "                \"weight_filler\": xavier(gain=\"relu\"),\n",
    "                \"bias_filler\": constant(0)\n",
    "            }\n",
    "    )\n",
    "    \"\"\"\n",
    "    fl = Flatten(graph, \"fl\",config={\n",
    "            \"dims\": 2\n",
    "    })\n",
    "    fc_8 = Dense(graph, \"fc_8\", config={\n",
    "        \"out\": 4096,\n",
    "        \"activation\": relu,\n",
    "        \"weight_filler\": xavier(),\n",
    "        \"bias_filler\": constant(0.1)\n",
    "    })\n",
    "    dp_8 = Dropout(graph, \"dp_8\")\n",
    "    fc_9 = Dense(graph, \"fc_9\", config={\n",
    "        \"out\": 19200,\n",
    "        \"activation\": relu,\n",
    "        \"weight_filler\": xavier(),\n",
    "        \"bias_filler\": constant(0.1)\n",
    "    })\n",
    "    dp_9 = Dropout(graph, \"dp_9\")\n",
    "    rs_10 = Reshape(graph, \"rs_10\", config={\n",
    "        \"shape\": (-1, 64, 15, 20)\n",
    "    })\n",
    "    \"\"\"\n",
    "    Expansive path\n",
    "    \"\"\"\n",
    "    up_11 = Upsample(graph, \"up_11\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_11 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_11\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_12 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_12\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_13 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_13\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    up_14 = Upsample(graph, \"up_14\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_14 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_14\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_15 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_15\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_16 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_16\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    up_17 = Upsample(graph, \"up_17\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_17 = Conv2D(graph, \"conv_17\", config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "    })\n",
    "    conv_18 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_18\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_19 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_19\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    up_20 = Upsample(graph, \"up_20\", config={\n",
    "        \"mode\": \"constant\",\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_20 = Conv2D(graph, \"conv_20\", config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "    })\n",
    "    conv_21 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_21\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_22 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_22\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_23 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_23\",\n",
    "        config={\n",
    "            \"channels\": 1,\n",
    "            \"kernel\": (1, 1),\n",
    "            \"activation\": None,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Feed forward nodes\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    concat_20 = Concatenate(graph, \"concat_20\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    concat_17 = Concatenate(graph, \"concat_17\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "    concat_14 = Concatenate(graph, \"concat_14\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    concat_11 = Concatenate(graph, \"concat_11\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Losses / Error\n",
    "    \"\"\"\n",
    "    loss = EuclideanLoss(graph, \"loss\")\n",
    "\n",
    "    error = MSE(graph, \"mse\", config={\n",
    "        \"root\": True,\n",
    "        \"is_output\": True,\n",
    "        \"phase\": PHASE_TRAIN\n",
    "    })\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Make connections\n",
    "    \"\"\"\n",
    "    data.connect(conv_1)\n",
    "    conv_1.connect(conv_2)\n",
    "    conv_2.connect(concat_20)\n",
    "    conv_2.connect(pool_2)\n",
    "    pool_2.connect(conv_3)\n",
    "    conv_3.connect(conv_4)\n",
    "    conv_4.connect(concat_17)\n",
    "    conv_4.connect(pool_4)\n",
    "    pool_4.connect(conv_5)\n",
    "    conv_5.connect(conv_6)\n",
    "    conv_6.connect(concat_14)\n",
    "    conv_6.connect(pool_6)\n",
    "    pool_6.connect(conv_7)\n",
    "    conv_7.connect(conv_8)\n",
    "    conv_8.connect(concat_11)\n",
    "    conv_8.connect(pool_8)\n",
    "    pool_8.connect(fl)\n",
    "    fl.connect(fc_8)\n",
    "    fc_8.connect(dp_8)\n",
    "    dp_8.connect(fc_9)\n",
    "    fc_9.connect(dp_9)\n",
    "    dp_9.connect(rs_10)\n",
    "    rs_10.connect(up_11)\n",
    "    up_11.connect(conv_11)\n",
    "    conv_11.connect(concat_11)\n",
    "    concat_11.connect(conv_12)\n",
    "    conv_12.connect(conv_13)\n",
    "    conv_13.connect(up_14)\n",
    "    up_14.connect(conv_14)\n",
    "    conv_14.connect(concat_14)\n",
    "    concat_14.connect(conv_15)\n",
    "    conv_15.connect(conv_16)\n",
    "    conv_16.connect(up_17)\n",
    "    up_17.connect(conv_17)\n",
    "    conv_17.connect(concat_17)\n",
    "    concat_17.connect(conv_18)\n",
    "    conv_18.connect(conv_19)\n",
    "    conv_19.connect(up_20)\n",
    "    up_20.connect(conv_20)\n",
    "    conv_20.connect(concat_20)\n",
    "    concat_20.connect(conv_21)\n",
    "    conv_21.connect(conv_22)\n",
    "    conv_22.connect(conv_23)\n",
    "\n",
    "    conv_23.connect(loss)\n",
    "    label.connect(loss)\n",
    "\n",
    "    conv_23.connect(error)\n",
    "    label.connect(error)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    batch_size = 4\n",
    "    chunk_size = 10*batch_size\n",
    "    transfer_shape = ((chunk_size, 3, 240, 320), (chunk_size, 240, 320))\n",
    "\n",
    "    g = build_u_graph()\n",
    "\n",
    "    # Build the training pipeline\n",
    "    db_loader = H5DBLoader(\"db\", ((chunk_size, 3, 480, 640), (chunk_size, 1, 480, 640)), config={\n",
    "        \"db\": \"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.hdf5\",\n",
    "        # \"db\": '../data/nyu_depth_unet_large.hdf5',\n",
    "        \"key_data\": \"images\",\n",
    "        \"key_label\": \"depths\",\n",
    "        \"chunk_size\": chunk_size\n",
    "    })\n",
    "    transformer = Transformer(\"tr\", transfer_shape, config={\n",
    "        # Measured empirically for the data-set\n",
    "        # \"offset\": 2.7321029\n",
    "        \"mean_file\": \"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.npy\",\n",
    "    })\n",
    "    optimizer = Optimizer(\"opt\", g, transfer_shape, config={\n",
    "        \"batch_size\":  batch_size,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        # \"learning_rate\": 0.000001,\n",
    "        \"learning_rate\": w\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.0005,\n",
    "        \"print_freq\": 50,\n",
    "        \"save_freq\": 3000,\n",
    "        # \"weights\": \"data/depth_pipeline_alexnet_test_noaug_iter_60000.zip\",\n",
    "        \"save_prefix\": \"../data/vnet2\"\n",
    "    })\n",
    "\n",
    "    p = Pipeline(config={\n",
    "        \"validation_frequency\": 20,\n",
    "        \"cycles\": 650\n",
    "    })\n",
    "    p.add(db_loader)\n",
    "    p.add(transformer)\n",
    "    p.add(optimizer)\n",
    "    p.run()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.80378\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n28zL5CUDwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f52f266c950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5pjoiYhi7v8B5HS/OJ8fgkUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f52f233acd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "l = np.array([s[\"loss\"] for s in optimizer.losses])\n",
    "e = np.array([s[\"mse\"] for s in optimizer.losses])\n",
    "print l.mean()\n",
    "plt.plot(l)\n",
    "plt.show()\n",
    "plt.plot(e)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
