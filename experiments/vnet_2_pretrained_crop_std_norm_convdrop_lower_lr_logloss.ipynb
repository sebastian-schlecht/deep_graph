{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from deepgraph.utils.logging import log\n",
    "from deepgraph.utils.common import batch_parallel, ConfigMixin, shuffle_in_unison_inplace, pickle_dump\n",
    "from deepgraph.utils.image import batch_pad_mirror\n",
    "from deepgraph.constants import *\n",
    "from deepgraph.conf import rng\n",
    "\n",
    "from deepgraph.pipeline import Processor, Packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepgraph.nn.init import *\n",
    "from deepgraph.graph import Node\n",
    "\n",
    "# Custom loss node\n",
    "class MaskedLogLoss(Node):\n",
    "    \"\"\"\n",
    "    Compute log scale invariant error for depth prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, graph, name, config={}):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param graph: Graph\n",
    "        :param name: String\n",
    "        :param config: Dict\n",
    "        :return: Node\n",
    "        \"\"\"\n",
    "        super(MaskedLogLoss, self).__init__(graph, name, config=config)\n",
    "        self.is_loss = True\n",
    "\n",
    "    def setup_defaults(self):\n",
    "        super(MaskedLogLoss, self).setup_defaults()\n",
    "        self.conf_default(\"loss_weight\", 1.0)\n",
    "        self.conf_default(\"lambda\", 0.5)\n",
    "        self.conf_default(\"label_index\", 1)\n",
    "\n",
    "    def alloc(self):\n",
    "            if len(self.inputs) != 2:\n",
    "                raise AssertionError(\"This node needs exactly two inputs to calculate loss.\")\n",
    "            self.output_shape = (1,)\n",
    "\n",
    "    def forward(self):\n",
    "            # Define our forward function\n",
    "            if self.conf(\"label_index\") == 1:\n",
    "                pred = self.inputs[0].expression\n",
    "                target = self.inputs[1].expression\n",
    "            else:\n",
    "                pred = self.inputs[1].expression\n",
    "                target = self.inputs[0].expression\n",
    "            \n",
    "            # Compute a mask which removes invalid values or 0\n",
    "            null_mask = T.gt(target,0)\n",
    "            # Compute a mask which removes the max values of a depth image\n",
    "            max_mask = T.lt(target, T.max(target))\n",
    "            # Combine masks\n",
    "            mask = null_mask * max_mask\n",
    "            # Now we compute the log of the input data stream, but neglect invalid values\n",
    "            log_target = T.switch(mask, T.log(target),0)\n",
    "            # We also apply the mask to our predictions in order to avoid computation of gradients on invalid pixels\n",
    "            masked_pred = mask * pred\n",
    "            # We compute the difference here to construct l_2 norm later in log space, neglecting invalid pixels\n",
    "            diff = log_target - masked_pred\n",
    "            # We also have to compute the mean only on valid pixels, so we need the number of valid pixels\n",
    "            n_valid = mask.sum()\n",
    "            # Finally compute the scale invariant error \n",
    "            self.expression = T.sum(diff**2) / n_valid - ((self.conf(\"lambda\") / (n_valid**2)) * (T.sum(diff)**2))\n",
    "            \n",
    "            \n",
    "            \n",
    "# Input data transformer\n",
    "class Transformer(Processor):\n",
    "    \"\"\"\n",
    "    Apply online random augmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, shapes, config, buffer_size=10):\n",
    "        super(Transformer, self).__init__(name, shapes, config, buffer_size)\n",
    "        self.mean = None\n",
    "\n",
    "    def init(self):\n",
    "        if self.conf(\"mean_file\") is not None:\n",
    "            self.mean = np.load(self.conf(\"mean_file\"))\n",
    "        else:\n",
    "            log(\"Transformer - No mean file specified.\", LOG_LEVEL_WARNING)\n",
    "\n",
    "    def process(self):\n",
    "        packet = self.pull()\n",
    "        # Return if no data is there\n",
    "        if not packet:\n",
    "            return False\n",
    "        # Unpack\n",
    "        data, label = packet.data\n",
    "        # Do processing\n",
    "        log(\"Transformer - Processing data\", LOG_LEVEL_VERBOSE)\n",
    "        \n",
    "        h = 240\n",
    "        w = 320\n",
    "        \n",
    "        start = time.time()\n",
    "        # Mean\n",
    "        if packet.phase == PHASE_TRAIN or packet.phase == PHASE_VAL:\n",
    "            data = data.astype(np.float32)\n",
    "            if self.mean is not None:\n",
    "                std = self.conf(\"std_norm\")\n",
    "                for idx in range(data.shape[0]):\n",
    "                    # Subtract mean\n",
    "                    data[idx] = data[idx] - self.mean.astype(np.float32)\n",
    "                    if std is not None:\n",
    "                        data[idx] =  data[idx] * std\n",
    "            if self.conf(\"offset\") is not None:\n",
    "                label -= self.conf(\"offset\")\n",
    "\n",
    "        if packet.phase == PHASE_TRAIN:\n",
    "             # Do elementwise operations\n",
    "            data_old = data\n",
    "            label_old = label\n",
    "            data = np.zeros((data_old.shape[0], data_old.shape[1], h, w), dtype=np.float32)\n",
    "            label = np.zeros((label_old.shape[0], h, w), dtype=np.float32)\n",
    "            for idx in range(data.shape[0]):\n",
    "                # Rotate\n",
    "                # We rotate before cropping to be able to get filled corners\n",
    "                # Maybe even adjust the border after rotating\n",
    "                deg = np.random.randint(-5,6)\n",
    "                # Operate on old data. Careful - data is already in float so we need to normalize and rescale afterwards\n",
    "                # data_old[idx] = 255. * rotate_transformer_rgb_uint8(data_old[idx] * 0.003921568627, deg).astype(np.float32)\n",
    "                # label_old[idx] = rotate_transformer_scalar_float32(label_old[idx], deg)\n",
    "                \n",
    "                # Take care of any empty areas, we crop on a smaller surface depending on the angle\n",
    "                # TODO Remove this once loss supports masking\n",
    "\n",
    "                shift = 0 #np.tan((deg/180.) * math.pi)\n",
    "                # Random crops\n",
    "                cy = rng.randint(data_old.shape[2] - h - shift, size=1)\n",
    "                cx = rng.randint(data_old.shape[3] - w - shift, size=1)\n",
    "\n",
    "                data[idx] = data_old[idx, :, cy:cy+h, cx:cx+w]\n",
    "                label[idx] = label_old[idx, cy:cy+h, cx:cx+w]\n",
    "\n",
    "                # Flip horizontally with probability 0.5\n",
    "                p = rng.randint(2)\n",
    "                if p > 0:\n",
    "                    data[idx] = data[idx, :, :, ::-1]\n",
    "                    label[idx] = label[idx, :, ::-1]\n",
    "\n",
    "                # RGB we mult with a random value between 0.8 and 1.2\n",
    "                r = rng.randint(80,121) / 100.\n",
    "                g = rng.randint(80,121) / 100.\n",
    "                b = rng.randint(80,121) / 100.\n",
    "                data[idx, 0] = data[idx, 0] * r\n",
    "                data[idx, 1] = data[idx, 1] * g\n",
    "                data[idx, 2] = data[idx, 2] * b\n",
    "                \n",
    "            # Shuffle\n",
    "            data, label = shuffle_in_unison_inplace(data, label)\n",
    "            \n",
    "        elif packet.phase == PHASE_VAL:\n",
    "            # Center crop\n",
    "            cy = (data.shape[2] - h) // 2\n",
    "            cx = (data.shape[3] - w) // 2\n",
    "            data = data[:, :, cy:cy+h, cx:cx+w]\n",
    "            label = label[:, cy:cy+h, cx:cx+w]\n",
    "            \n",
    "        end = time.time()\n",
    "        log(\"Transformer - Processing took \" + str(end - start) + \" seconds.\", LOG_LEVEL_VERBOSE)\n",
    "        # Try to push into queue as long as thread should not terminate\n",
    "        self.push(Packet(identifier=packet.id, phase=packet.phase, num=2, data=(data, label)))\n",
    "        return True\n",
    "\n",
    "    def setup_defaults(self):\n",
    "        super(Transformer, self).setup_defaults()\n",
    "        self.conf_default(\"mean_file\", None)\n",
    "        self.conf_default(\"offset\", None)\n",
    "        self.conf_default(\"std_norm\", 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from theano.tensor.nnet import relu\n",
    "\n",
    "from deepgraph.graph import *\n",
    "from deepgraph.nn.core import *\n",
    "from deepgraph.nn.conv import *\n",
    "from deepgraph.nn.loss import *\n",
    "from deepgraph.solver import *\n",
    "from deepgraph.nn.init import *\n",
    "\n",
    "from deepgraph.pipeline import Optimizer, H5DBLoader, Pipeline\n",
    "\n",
    "\n",
    "def build_u_graph():\n",
    "    graph = Graph(\"u_depth\")\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    \"\"\"\n",
    "    data = Data(graph, \"data\", T.ftensor4, shape=(-1, 3, 240, 320))\n",
    "    label = Data(graph, \"label\", T.ftensor3, shape=(-1, 1, 240, 320), config={\n",
    "        \"phase\": PHASE_TRAIN\n",
    "    })\n",
    "    \"\"\"\n",
    "    Contractive part\n",
    "    \"\"\"\n",
    "    conv_1 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_1\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_2 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_2\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    pool_2 = Pool(graph, \"pool_2\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_3 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_3\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_4 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_4\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    pool_4 = Pool(graph, \"pool_4\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "\n",
    "    conv_5 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_5\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_6 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_6\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    pool_6 = Pool(graph, \"pool_6\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "\n",
    "    conv_7 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_7\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_8 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_8\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    dp_c8  = Dropout(graph, \"dpc_8\")\n",
    "    pool_8 = Pool(graph, \"pool_8\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    fl = Flatten(graph, \"fl\",config={\n",
    "            \"dims\": 2\n",
    "    })\n",
    "    fc_8 = Dense(graph, \"fc_8\", config={\n",
    "        \"out\": 4096,\n",
    "        \"activation\": relu,\n",
    "        \"weight_filler\": xavier(),\n",
    "        \"bias_filler\": constant(0.1)\n",
    "    })\n",
    "    dp_8 = Dropout(graph, \"dp_8\")\n",
    "    fc_9 = Dense(graph, \"fc_9\", config={\n",
    "        \"out\": 19200,\n",
    "        \"activation\": relu,\n",
    "        \"weight_filler\": xavier(),\n",
    "        \"bias_filler\": constant(0.1)\n",
    "    })\n",
    "    dp_9 = Dropout(graph, \"dp_9\")\n",
    "    rs_10 = Reshape(graph, \"rs_10\", config={\n",
    "        \"shape\": (-1, 64, 15, 20)\n",
    "    })\n",
    "    \"\"\"\n",
    "    Expansive path\n",
    "    \"\"\"\n",
    "    up_11 = Upsample(graph, \"up_11\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_11 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_11\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_12 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_12\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_13 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_13\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    up_14 = Upsample(graph, \"up_14\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_14 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_14\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_15 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_15\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_16 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_16\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    up_17 = Upsample(graph, \"up_17\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_17 = Conv2D(graph, \"conv_17\", config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "    })\n",
    "    conv_18 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_18\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_19 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_19\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    up_20 = Upsample(graph, \"up_20\", config={\n",
    "        \"mode\": \"constant\",\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_20 = Conv2D(graph, \"conv_20\", config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "    })\n",
    "    conv_21 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_21\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_22 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_22\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0.0001)\n",
    "        }\n",
    "    )\n",
    "    conv_23 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_23_with_bias\",\n",
    "        config={\n",
    "            \"channels\": 1,\n",
    "            \"kernel\": (1, 1),\n",
    "            \"activation\": None,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0.0001),\n",
    "            \"is_output\": True\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Feed forward nodes\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    concat_20 = Concatenate(graph, \"concat_20\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    concat_17 = Concatenate(graph, \"concat_17\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "    concat_14 = Concatenate(graph, \"concat_14\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    concat_11 = Concatenate(graph, \"concat_11\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Losses / Error\n",
    "    \"\"\"\n",
    "    loss = MaskedLogLoss(graph, \"loss\")\n",
    "\n",
    "    error = MSE(graph, \"mse\", config={\n",
    "        \"root\": True,\n",
    "        \"is_output\": True,\n",
    "        \"phase\": PHASE_TRAIN\n",
    "    })\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Make connections\n",
    "    \"\"\"\n",
    "    data.connect(conv_1)\n",
    "    conv_1.connect(conv_2)\n",
    "    conv_2.connect(concat_20)\n",
    "    conv_2.connect(pool_2)\n",
    "    pool_2.connect(conv_3)\n",
    "    conv_3.connect(conv_4)\n",
    "    conv_4.connect(concat_17)\n",
    "    conv_4.connect(pool_4)\n",
    "    pool_4.connect(conv_5)\n",
    "    conv_5.connect(conv_6)\n",
    "    conv_6.connect(concat_14)\n",
    "    conv_6.connect(pool_6)\n",
    "    pool_6.connect(conv_7)\n",
    "    conv_7.connect(conv_8)\n",
    "    conv_8.connect(concat_11)\n",
    "    conv_8.connect(dp_c8)\n",
    "    dp_c8.connect(pool_8)\n",
    "    pool_8.connect(fl)\n",
    "    fl.connect(fc_8)\n",
    "    fc_8.connect(dp_8)\n",
    "    dp_8.connect(fc_9)\n",
    "    fc_9.connect(dp_9)\n",
    "    dp_9.connect(rs_10)\n",
    "    rs_10.connect(up_11)\n",
    "    up_11.connect(conv_11)\n",
    "    conv_11.connect(concat_11)\n",
    "    concat_11.connect(conv_12)\n",
    "    conv_12.connect(conv_13)\n",
    "    conv_13.connect(up_14)\n",
    "    up_14.connect(conv_14)\n",
    "    conv_14.connect(concat_14)\n",
    "    concat_14.connect(conv_15)\n",
    "    conv_15.connect(conv_16)\n",
    "    conv_16.connect(up_17)\n",
    "    up_17.connect(conv_17)\n",
    "    conv_17.connect(concat_17)\n",
    "    concat_17.connect(conv_18)\n",
    "    conv_18.connect(conv_19)\n",
    "    conv_19.connect(up_20)\n",
    "    up_20.connect(conv_20)\n",
    "    conv_20.connect(concat_20)\n",
    "    concat_20.connect(conv_21)\n",
    "    conv_21.connect(conv_22)\n",
    "    conv_22.connect(conv_23)\n",
    "    \n",
    "    # Make sure we connect the prediction first and then the loss to compute the log space properly\n",
    "    conv_23.connect(loss)\n",
    "    label.connect(loss)\n",
    "\n",
    "    conv_23.connect(error)\n",
    "    label.connect(error)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    batch_size = 4\n",
    "    chunk_size = 10*batch_size\n",
    "    transfer_shape = ((chunk_size, 3, 240, 320), (chunk_size, 240, 320))\n",
    "\n",
    "    g = build_u_graph()\n",
    "\n",
    "    # Build the training pipeline\n",
    "    db_loader = H5DBLoader(\"db\", ((chunk_size, 3, 480, 640), (chunk_size, 1, 480, 640)), config={\n",
    "        \"db\": \"/home/ga29mix/nashome/data/nyu_depth_combined_vnet2.hdf5\",\n",
    "        # \"db\": '../data/nyu_depth_unet_large.hdf5',\n",
    "        \"key_data\": \"images\",\n",
    "        \"key_label\": \"depths\",\n",
    "        \"chunk_size\": chunk_size\n",
    "    })\n",
    "    transformer = Transformer(\"tr\", transfer_shape, config={\n",
    "        # Pixel & channelwise mean \n",
    "        \"mean_file\": \"/home/ga29mix/nashome/data/nyu_depth_combined_vnet2.npy\",\n",
    "        # 1 / sqrt(var(data))\n",
    "        \"std_norm\": 1.0 / 72.240064849107824\n",
    "    })\n",
    "    optimizer = Optimizer(\"opt\", g, transfer_shape, config={\n",
    "        \"batch_size\":  batch_size,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"learning_rate\": 0.01, # for step 1\n",
    "        # \"learning_rate\": 0.00001, # for step 2\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.0005,\n",
    "        \"print_freq\": 20,\n",
    "        \"save_freq\": 50000,\n",
    "        # \"weights\": \"/data/vnet2_crop_convdrop_logloss_iter_50000.zip\",\n",
    "        \"save_prefix\": \"/data/vnet2_crop_convdrop_logloss\"\n",
    "    })\n",
    "\n",
    "    p = Pipeline(config={\n",
    "        \"validation_frequency\": 20,\n",
    "        \"cycles\": 15500\n",
    "    })\n",
    "    p.add(db_loader)\n",
    "    p.add(transformer)\n",
    "    p.add(optimizer)\n",
    "    p.run()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "l = np.array([s[\"loss\"] for s in optimizer.losses])\n",
    "e = np.array([s[\"mse\"] for s in optimizer.losses])\n",
    "print l.mean()\n",
    "plt.plot(l)\n",
    "plt.show()\n",
    "plt.plot(e)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py, numpy as np\n",
    "f = h5py.File(\"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.hdf5\")\n",
    "b = int(f[\"images\"].shape[0] * 0.9)\n",
    "images = np.array(f[\"images\"][0:200])\n",
    "depths = np.array(f[\"depths\"][0:200])\n",
    "print images.shape\n",
    "mean = np.load(\"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = build_u_graph()\n",
    "g.load_weights(\"/data/vnet2_crop_convdrop_logloss_iter_50000.zip\")\n",
    "g.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from deepgraph.nn.core import Dropout\n",
    "\n",
    "plot = True\n",
    "idx = 0\n",
    "diffs = []\n",
    "Dropout.set_dp_off()\n",
    "for image in images:\n",
    "    tmp = image.astype(np.float32)\n",
    "    tmp -= mean\n",
    "    tmp *= 1.0 / 72.240064849107824\n",
    "    \n",
    "    res = g.infer([tmp.reshape((1,3,240,320))])[\"conv_23_with_bias\"]\n",
    "    res = res.squeeze()\n",
    "    res = np.exp(res)\n",
    "    depth = depths[idx]\n",
    "    if plot and idx % 4 == 0:\n",
    "        \n",
    "        plt.imshow(image.transpose((1,2,0)).astype(np.uint8))\n",
    "        plt.show()\n",
    "        plt.imshow(depth)\n",
    "        plt.show()\n",
    "        plt.imshow(res)\n",
    "        plt.show()\n",
    "        print \"RMSE: \" + str(np.sqrt(np.mean((res-depth)**2)))\n",
    "    diffs.append(res - depth)\n",
    "    \n",
    "    idx += 1\n",
    "    \n",
    "diffs = np.array(diffs)\n",
    "rmse = np.sqrt(np.mean(diffs ** 2))\n",
    "print \"Accumulated RMSE: \" + str(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer random images from the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib, cStringIO\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from deepgraph.nn.core import Dropout\n",
    "\n",
    "\n",
    "Dropout.set_dp_off()\n",
    "\n",
    "\n",
    "urls = [\n",
    "    'http://www.hotel-im-wasserturm.de/fileadmin/_processed_/csm_Deluxe_Doppelzimmer_2_6455fa52be.jpg',\n",
    "    'http://hotel-airport-zuerich.dorint.com/fileadmin/_processed_/csm_ZRH_Zimmer_DZ_04_bc3a12f87e.jpg',\n",
    "    'http://www.erlebnis-erlensee.de/images/bilder/partyservice/schnitzel.jpg',\n",
    "    'http://www.hankewitz.com/wp-content/uploads/2012/12/bolognese21.jpg'\n",
    "       ]\n",
    "for URL in urls:\n",
    "    file = cStringIO.StringIO(urllib.urlopen(URL).read())\n",
    "    img = Image.open(file)\n",
    "\n",
    "    rs = img.resize((320,240))\n",
    "    mean = np.load(\"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.npy\")\n",
    "    arr = np.array(rs).transpose((2,0,1)).astype(np.float32)\n",
    "    arr -= mean\n",
    "    arr *= 1.0 / 72.240064849107824\n",
    "    res = g.infer([arr.reshape((1,3,240,320))])[\"conv_23_with_bias\"]\n",
    "    res = np.exp(res)\n",
    "    # Montage to manipulate color scale\n",
    "    mmax = 8.0\n",
    "    # res[0,0,0,0] = 0.0\n",
    "    # res[0,0,0,1] = mmax\n",
    "    plt.imshow(rs)\n",
    "    plt.show()\n",
    "    plt.imshow(res.squeeze().clip(0.0,mmax))\n",
    "    plt.show()\n",
    "\n",
    "    print res.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer image from food data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib, cStringIO\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from deepgraph.nn.core import Dropout\n",
    "import numpy as np\n",
    "\n",
    "Dropout.set_dp_off()\n",
    "\n",
    "data = np.load('/home/ga29mix/nashome/data/kinectv2_2016_03_30_13_33_25_bgr.npy')[:,:,0:3]\n",
    "dd = np.load('/home/ga29mix/nashome/data/kinectv2_2016_03_30_13_33_25_depth.npy')\n",
    "\n",
    "img = Image.fromarray(np.array(data[:,:,::-1]))\n",
    "rs = img.resize((320,240))\n",
    "mean = np.load(\"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.npy\")\n",
    "arr = np.array(rs).transpose((2,0,1)).astype(np.float32)\n",
    "arr -= mean\n",
    "arr *= 1.0 / 72.240064849107824\n",
    "res = g.infer([arr.reshape((1,3,240,320))])[\"conv_23_with_bias\"]\n",
    "res = np.exp(res)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.imshow(res.squeeze())\n",
    "plt.show()\n",
    "plt.imshow(dd)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
