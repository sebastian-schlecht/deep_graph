{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  _____                _____                 _\n",
      " |  _  \\              |  __ \\               | |\n",
      " | | | |___  ___ _ __ | |  \\/_ __ __ _ _ __ | |__\n",
      " | | | / _ \\/ _ \\ '_ \\| | __| '__/ _` | '_ \\| '_ \\\n",
      " | |/ /  __/  __/ |_) | |_\\ \\ | | (_| | |_) | | | |\n",
      " |___/ \\___|\\___| .__/ \\____/_|  \\__,_| .__/|_| |_|\n",
      "                | |                   | |\n",
      "                |_|                   |_|\n",
      "\n",
      "\n",
      "Available on GitHub: https://github.com/sebastian-schlecht/deepgraph\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled with initial size: 90.0% of memory, cuDNN Version is too old. Update to v5, was 2000.)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from deepgraph.utils.logging import log\n",
    "from deepgraph.utils.common import batch_parallel, ConfigMixin, shuffle_in_unison_inplace, pickle_dump\n",
    "from deepgraph.utils.image import batch_pad_mirror\n",
    "from deepgraph.constants import *\n",
    "from deepgraph.conf import rng\n",
    "\n",
    "from deepgraph.pipeline import Processor, Packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepgraph.nn.init import *\n",
    "from deepgraph.graph import Node\n",
    "\n",
    "# Custom loss node\n",
    "class MaskedLogLoss(Node):\n",
    "    \"\"\"\n",
    "    Compute log scale invariant error for depth prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, graph, name, config={}):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param graph: Graph\n",
    "        :param name: String\n",
    "        :param config: Dict\n",
    "        :return: Node\n",
    "        \"\"\"\n",
    "        super(MaskedLogLoss, self).__init__(graph, name, config=config)\n",
    "        self.is_loss = True\n",
    "\n",
    "    def setup_defaults(self):\n",
    "        super(MaskedLogLoss, self).setup_defaults()\n",
    "        self.conf_default(\"loss_weight\", 1.0)\n",
    "        self.conf_default(\"lambda\", 0.2)\n",
    "        self.conf_default(\"label_index\", 1)\n",
    "\n",
    "    def alloc(self):\n",
    "            if len(self.inputs) != 2:\n",
    "                raise AssertionError(\"This node needs exactly two inputs to calculate loss.\")\n",
    "            self.output_shape = (1,)\n",
    "\n",
    "    def forward(self):\n",
    "            # Define our forward function\n",
    "            if self.conf(\"label_index\") == 1:\n",
    "                pred = self.inputs[0].expression\n",
    "                target = self.inputs[1].expression\n",
    "            else:\n",
    "                pred = self.inputs[1].expression\n",
    "                target = self.inputs[0].expression\n",
    "            \n",
    "            # Compute a mask which removes invalid values or 0\n",
    "            null_mask = T.gt(target,0)\n",
    "            # Compute a mask which removes the max values of a depth image\n",
    "            max_mask = T.lt(target, T.max(target))\n",
    "            # Min mask\n",
    "            min_mask = T.gt(target, T.min(target))\n",
    "            # Combine masks\n",
    "            mask = null_mask * max_mask * min_mask\n",
    "            # Now we compute the log of the input data stream, but neglect invalid values\n",
    "            log_target = T.switch(mask, T.log(target),0)\n",
    "            # We also apply the mask to our predictions in order to avoid computation of gradients on invalid pixels\n",
    "            masked_pred = mask * pred\n",
    "            # We compute the difference here to construct l_2 norm later in log space, neglecting invalid pixels\n",
    "            diff = log_target - masked_pred\n",
    "            # We also have to compute the mean only on valid pixels, so we need the number of valid pixels\n",
    "            n_valid = mask.sum()\n",
    "            # Finally compute the scale invariant error \n",
    "            self.expression = T.sum(diff**2) / n_valid - ((self.conf(\"lambda\") / (n_valid**2)) * (T.sum(diff)**2))\n",
    "            \n",
    "            \n",
    "            \n",
    "# Input data transformer\n",
    "class Transformer(Processor):\n",
    "    \"\"\"\n",
    "    Apply online random augmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, shapes, config, buffer_size=10):\n",
    "        super(Transformer, self).__init__(name, shapes, config, buffer_size)\n",
    "        self.mean = None\n",
    "\n",
    "    def init(self):\n",
    "        if self.conf(\"mean_file\") is not None:\n",
    "            self.mean = np.load(self.conf(\"mean_file\"))\n",
    "        else:\n",
    "            log(\"Transformer - No mean file specified.\", LOG_LEVEL_WARNING)\n",
    "\n",
    "    def process(self):\n",
    "        packet = self.pull()\n",
    "        # Return if no data is there\n",
    "        if not packet:\n",
    "            return False\n",
    "        # Unpack\n",
    "        data, label = packet.data\n",
    "        # Do processing\n",
    "        log(\"Transformer - Processing data\", LOG_LEVEL_VERBOSE)\n",
    "        \n",
    "        h = 240\n",
    "        w = 320\n",
    "        \n",
    "        start = time.time()\n",
    "        # Mean\n",
    "        if packet.phase == PHASE_TRAIN or packet.phase == PHASE_VAL:\n",
    "            data = data.astype(np.float32)\n",
    "            if self.mean is not None:\n",
    "                std = self.conf(\"std_norm\")\n",
    "                for idx in range(data.shape[0]):\n",
    "                    # Subtract mean\n",
    "                    data[idx] = data[idx] - self.mean.astype(np.float32)\n",
    "                    if std is not None:\n",
    "                        data[idx] =  data[idx] * std\n",
    "            if self.conf(\"offset\") is not None:\n",
    "                label -= self.conf(\"offset\")\n",
    "            if self.conf(\"pred_std_norm\") is not None:\n",
    "                label *= self.conf(\"pred_std_norm\")\n",
    "\n",
    "        if packet.phase == PHASE_TRAIN:\n",
    "             # Do elementwise operations\n",
    "            data_old = data\n",
    "            label_old = label\n",
    "            data = np.zeros((data_old.shape[0], data_old.shape[1], h, w), dtype=np.float32)\n",
    "            label = np.zeros((label_old.shape[0], h, w), dtype=np.float32)\n",
    "            for idx in range(data.shape[0]):\n",
    "                # Rotate\n",
    "                # We rotate before cropping to be able to get filled corners\n",
    "                # Maybe even adjust the border after rotating\n",
    "                deg = np.random.randint(-5,6)\n",
    "                # Operate on old data. Careful - data is already in float so we need to normalize and rescale afterwards\n",
    "                # data_old[idx] = 255. * rotate_transformer_rgb_uint8(data_old[idx] * 0.003921568627, deg).astype(np.float32)\n",
    "                # label_old[idx] = rotate_transformer_scalar_float32(label_old[idx], deg)\n",
    "                \n",
    "                # Take care of any empty areas, we crop on a smaller surface depending on the angle\n",
    "                # TODO Remove this once loss supports masking\n",
    "\n",
    "                shift = 0 #np.tan((deg/180.) * math.pi)\n",
    "                # Random crops\n",
    "                cy = rng.randint(data_old.shape[2] - h - shift, size=1)\n",
    "                cx = rng.randint(data_old.shape[3] - w - shift, size=1)\n",
    "\n",
    "                data[idx] = data_old[idx, :, cy:cy+h, cx:cx+w]\n",
    "                label[idx] = label_old[idx, cy:cy+h, cx:cx+w]\n",
    "\n",
    "                # Flip horizontally with probability 0.5\n",
    "                p = rng.randint(2)\n",
    "                if p > 0:\n",
    "                    data[idx] = data[idx, :, :, ::-1]\n",
    "                    label[idx] = label[idx, :, ::-1]\n",
    "\n",
    "                # RGB we mult with a random value between 0.8 and 1.2\n",
    "                r = rng.randint(80,121) / 100.\n",
    "                g = rng.randint(80,121) / 100.\n",
    "                b = rng.randint(80,121) / 100.\n",
    "                data[idx, 0] = data[idx, 0] * r\n",
    "                data[idx, 1] = data[idx, 1] * g\n",
    "                data[idx, 2] = data[idx, 2] * b\n",
    "                \n",
    "            # Shuffle\n",
    "            data, label = shuffle_in_unison_inplace(data, label)\n",
    "            \n",
    "        elif packet.phase == PHASE_VAL:\n",
    "            # Center crop\n",
    "            cy = (data.shape[2] - h) // 2\n",
    "            cx = (data.shape[3] - w) // 2\n",
    "            data = data[:, :, cy:cy+h, cx:cx+w]\n",
    "            label = label[:, cy:cy+h, cx:cx+w]\n",
    "            \n",
    "        end = time.time()\n",
    "        log(\"Transformer - Processing took \" + str(end - start) + \" seconds.\", LOG_LEVEL_VERBOSE)\n",
    "        # Try to push into queue as long as thread should not terminate\n",
    "        self.push(Packet(identifier=packet.id, phase=packet.phase, num=2, data=(data, label)))\n",
    "        return True\n",
    "\n",
    "    def setup_defaults(self):\n",
    "        super(Transformer, self).setup_defaults()\n",
    "        self.conf_default(\"mean_file\", None)\n",
    "        self.conf_default(\"offset\", None)\n",
    "        self.conf_default(\"std_norm\", 1.0)\n",
    "        self.conf_default(\"pred_std_norm\",1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano.tensor.nnet import relu\n",
    "\n",
    "from deepgraph.graph import *\n",
    "from deepgraph.nn.core import *\n",
    "from deepgraph.nn.conv import *\n",
    "from deepgraph.nn.loss import *\n",
    "from deepgraph.solver import *\n",
    "from deepgraph.nn.init import *\n",
    "\n",
    "from deepgraph.pipeline import Optimizer, H5DBLoader, Pipeline\n",
    "\n",
    "\n",
    "def build_u_graph():\n",
    "    # Disable CUDNN\n",
    "    # TODO Remove\n",
    "    Conv2D.use_cudnn = False\n",
    "    Pool.use_cudnn = False\n",
    "    graph = Graph(\"u_depth\")\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    \"\"\"\n",
    "    data = Data(graph, \"data\", T.ftensor4, shape=(-1, 3, 240, 320))\n",
    "    label = Data(graph, \"label\", T.ftensor3, shape=(-1, 1, 240, 320), config={\n",
    "        \"phase\": PHASE_TRAIN\n",
    "    })\n",
    "    \"\"\"\n",
    "    Contractive part\n",
    "    \"\"\"\n",
    "    conv_1 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_1\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_2 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_2\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    pool_2 = Pool(graph, \"pool_2\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_3 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_3\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_4 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_4\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    pool_4 = Pool(graph, \"pool_4\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "\n",
    "    conv_5 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_5\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_6 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_6\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    pool_6 = Pool(graph, \"pool_6\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "\n",
    "    conv_7 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_7\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_8 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_8\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    dp_c8  = Dropout(graph, \"dpc_8\")\n",
    "    pool_8 = Pool(graph, \"pool_8\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    fl = Flatten(graph, \"fl\",config={\n",
    "            \"dims\": 2\n",
    "    })\n",
    "    fc_8 = Dense(graph, \"fc_8\", config={\n",
    "        \"out\": 4096,\n",
    "        \"activation\": relu,\n",
    "        \"weight_filler\": xavier(),\n",
    "        \"bias_filler\": constant(0.01)\n",
    "    })\n",
    "    dp_8 = Dropout(graph, \"dp_8\")\n",
    "    fc_9 = Dense(graph, \"fc_9\", config={\n",
    "        \"out\": 19200,\n",
    "        \"activation\": relu,\n",
    "        \"weight_filler\": xavier(),\n",
    "        \"bias_filler\": constant(0.01)\n",
    "    })\n",
    "    dp_9 = Dropout(graph, \"dp_9\")\n",
    "    rs_10 = Reshape(graph, \"rs_10\", config={\n",
    "        \"shape\": (-1, 64, 15, 20)\n",
    "    })\n",
    "    \"\"\"\n",
    "    Expansive path\n",
    "    \"\"\"\n",
    "    up_11 = Upsample(graph, \"up_11\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_11 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_11\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_12 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_12\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_13 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_13\",\n",
    "        config={\n",
    "            \"channels\": 512,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    up_14 = Upsample(graph, \"up_14\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_14 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_14\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_15 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_15_b\", # Renamed to prevent weight filler from-file init\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_16 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_16\",\n",
    "        config={\n",
    "            \"channels\": 256,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    up_17 = Upsample(graph, \"up_17\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_17 = Conv2D(graph, \"conv_17\", config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "    })\n",
    "    conv_18 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_18\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_19 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_19\",\n",
    "        config={\n",
    "            \"channels\": 128,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    up_20 = Upsample(graph, \"up_20\", config={\n",
    "        \"mode\": \"constant\",\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    conv_20 = Conv2D(graph, \"conv_20\", config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": 1,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0)\n",
    "    })\n",
    "    conv_21 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_21\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_22 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_22\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (3, 3),\n",
    "            \"border_mode\": (1, 1),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    conv_23 = Conv2D(\n",
    "        graph,\n",
    "        \"conv_23_with_bias\",\n",
    "        config={\n",
    "            \"channels\": 1,\n",
    "            \"kernel\": (1, 1),\n",
    "            \"activation\": None,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"weight_filler\": xavier(),\n",
    "            \"bias_filler\": constant(0.0001),\n",
    "            \"is_output\": True\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Feed forward nodes\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    concat_20 = Concatenate(graph, \"concat_20\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    concat_17 = Concatenate(graph, \"concat_17\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "    concat_14 = Concatenate(graph, \"concat_14\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \n",
    "\n",
    "    concat_11 = Concatenate(graph, \"concat_11\", config={\n",
    "        \"axis\": 1\n",
    "    })\n",
    "\n",
    "    \"\"\"\n",
    "    Large scale feature refinement path\n",
    "    \"\"\"\n",
    "    ls_conv_1 = Conv2D(\n",
    "        graph,\n",
    "        \"ls_conv_1\",\n",
    "        config={\n",
    "            \"channels\": 96,\n",
    "            \"kernel\": (9, 9),\n",
    "            \"border_mode\": (4, 4),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    ls_pool_1 = Pool(graph, \"ls_pool_1\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    ls_conv_2 = Conv2D(\n",
    "        graph,\n",
    "        \"ls_conv_2\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (5, 5),\n",
    "            \"border_mode\": (2, 2),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    ls_conv_3 = Conv2D(\n",
    "        graph,\n",
    "        \"ls_conv_3\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (5, 5),\n",
    "            \"border_mode\": (2, 2),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    ls_pool_3 = Pool(graph, \"ls_pool_3\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    ls_conv_4 = Conv2D(\n",
    "        graph,\n",
    "        \"ls_conv_4\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (5, 5),\n",
    "            \"border_mode\": (2, 2),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    \"\"\"\n",
    "    Mid scale feature refinement path\n",
    "    \"\"\"\n",
    "    \n",
    "    ms_conv_1 = Conv2D(\n",
    "        graph,\n",
    "        \"ms_conv_1\",\n",
    "        config={\n",
    "            \"channels\": 96,\n",
    "            \"kernel\": (9, 9),\n",
    "            \"border_mode\": (4, 4),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    ms_pool_1 = Pool(graph, \"ms_pool_1\", config={\n",
    "        \"kernel\": (2, 2)\n",
    "    })\n",
    "    ms_conv_2 = Conv2D(\n",
    "        graph,\n",
    "        \"ms_conv_2\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (5, 5),\n",
    "            \"border_mode\": (2, 2),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    ms_conv_3 = Conv2D(\n",
    "        graph,\n",
    "        \"ms_conv_3\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (5, 5),\n",
    "            \"border_mode\": (2, 2),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    ms_conv_4 = Conv2D(\n",
    "        graph,\n",
    "        \"ms_conv_4\",\n",
    "        config={\n",
    "            \"channels\": 64,\n",
    "            \"kernel\": (5, 5),\n",
    "            \"border_mode\": (2, 2),\n",
    "            \"activation\": relu,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"weight_filler\": xavier(gain=\"relu\"),\n",
    "            \"bias_filler\": constant(0)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Losses / Error\n",
    "    \"\"\"\n",
    "    loss = EuclideanLoss(graph, \"loss\")\n",
    "\n",
    "    error = MSE(graph, \"mse\", config={\n",
    "        \"root\": True,\n",
    "        \"is_output\": True,\n",
    "        \"phase\": PHASE_TRAIN\n",
    "    })\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Make connections\n",
    "    \"\"\"\n",
    "    \n",
    "    # Contractive path with feed forward 3x3 kernel paths\n",
    "    data.connect(conv_1)\n",
    "    conv_1.connect(conv_2)\n",
    "    conv_2.connect(concat_20)\n",
    "    conv_2.connect(pool_2)\n",
    "    pool_2.connect(conv_3)\n",
    "    conv_3.connect(conv_4)\n",
    "    conv_4.connect(concat_17)\n",
    "    conv_4.connect(pool_4)\n",
    "    pool_4.connect(conv_5)\n",
    "    conv_5.connect(conv_6)\n",
    "    conv_6.connect(concat_14)\n",
    "    conv_6.connect(pool_6)\n",
    "    pool_6.connect(conv_7)\n",
    "    conv_7.connect(conv_8)\n",
    "    conv_8.connect(concat_11)\n",
    "    conv_8.connect(dp_c8)\n",
    "    dp_c8.connect(pool_8)\n",
    "    pool_8.connect(fl)\n",
    "    fl.connect(fc_8)\n",
    "    fc_8.connect(dp_8)\n",
    "    dp_8.connect(fc_9)\n",
    "    fc_9.connect(dp_9)\n",
    "    dp_9.connect(rs_10)\n",
    "    rs_10.connect(up_11)\n",
    "    up_11.connect(conv_11)\n",
    "    conv_11.connect(concat_11)\n",
    "    concat_11.connect(conv_12)\n",
    "    conv_12.connect(conv_13)\n",
    "    conv_13.connect(up_14)\n",
    "    up_14.connect(conv_14)\n",
    "    conv_14.connect(concat_14)\n",
    "    concat_14.connect(conv_15)\n",
    "    conv_15.connect(conv_16)\n",
    "    conv_16.connect(up_17)\n",
    "    up_17.connect(conv_17)\n",
    "    conv_17.connect(concat_17)\n",
    "    concat_17.connect(conv_18)\n",
    "    conv_18.connect(conv_19)\n",
    "    conv_19.connect(up_20)\n",
    "    up_20.connect(conv_20)\n",
    "    conv_20.connect(concat_20)\n",
    "    concat_20.connect(conv_21)\n",
    "    conv_21.connect(conv_22)\n",
    "    conv_22.connect(conv_23)\n",
    "    \n",
    "    # Connect mid scale, large feature path\n",
    "    data.connect(ms_conv_1)\n",
    "    ms_conv_1.connect(ms_pool_1)\n",
    "    ms_pool_1.connect(ms_conv_2)\n",
    "    ms_conv_2.connect(ms_conv_3)\n",
    "    ms_conv_3.connect(ms_conv_4)\n",
    "    ms_conv_4.connect(concat_17)\n",
    "    \n",
    "    # Connect large scale, larger feature path\n",
    "    data.connect(ls_conv_1)\n",
    "    ls_conv_1.connect(ls_pool_1)\n",
    "    ls_pool_1.connect(ls_conv_2)\n",
    "    ls_conv_2.connect(ls_conv_3)\n",
    "    ls_conv_3.connect(ls_pool_3)\n",
    "    ls_pool_3.connect(ls_conv_4)\n",
    "    ls_conv_4.connect(concat_14)\n",
    "    \n",
    "    # Make sure we connect the prediction first and then the loss to compute the log space properly\n",
    "    conv_23.connect(loss)\n",
    "    label.connect(loss)\n",
    "\n",
    "    conv_23.connect(error)\n",
    "    label.connect(error)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-05-02 15:15:27] INFO: H5DBLoader - Caching DB in memory\n",
      "[2016-05-02 15:15:27] INFO: Graph - Setting up graph\n",
      "[2016-05-02 15:15:27] INFO: Node - data has shape (-1, 3, 240, 320)\n",
      "[2016-05-02 15:15:27] INFO: Node - label has shape (-1, 1, 240, 320)\n",
      "[2016-05-02 15:15:27] INFO: Node - conv_1 has shape (-1, 64, 240, 320)\n",
      "[2016-05-02 15:15:27] INFO: Node - ms_conv_1 has shape (-1, 96, 240, 320)\n",
      "[2016-05-02 15:15:27] INFO: Node - ls_conv_1 has shape (-1, 96, 240, 320)\n",
      "[2016-05-02 15:15:27] INFO: Node - conv_2 has shape (-1, 64, 240, 320)\n",
      "[2016-05-02 15:15:27] INFO: Node - pool_2 has shape (-1, 64, 120, 160)\n",
      "[2016-05-02 15:15:27] INFO: Node - conv_3 has shape (-1, 128, 120, 160)\n",
      "[2016-05-02 15:15:27] INFO: Node - conv_4 has shape (-1, 128, 120, 160)\n",
      "[2016-05-02 15:15:27] INFO: Node - pool_4 has shape (-1, 128, 60, 80)\n",
      "[2016-05-02 15:15:27] INFO: Node - conv_5 has shape (-1, 256, 60, 80)\n",
      "[2016-05-02 15:15:27] INFO: Node - conv_6 has shape (-1, 256, 60, 80)\n",
      "[2016-05-02 15:15:27] INFO: Node - pool_6 has shape (-1, 256, 30, 40)\n",
      "[2016-05-02 15:15:27] INFO: Node - conv_7 has shape (-1, 512, 30, 40)\n",
      "[2016-05-02 15:15:27] INFO: Node - conv_8 has shape (-1, 512, 30, 40)\n",
      "[2016-05-02 15:15:27] INFO: Node - dpc_8 has shape (-1, 512, 30, 40)\n",
      "[2016-05-02 15:15:27] INFO: Node - pool_8 has shape (-1, 512, 15, 20)\n",
      "[2016-05-02 15:15:27] INFO: Node - fl has shape (-1, 153600)\n",
      "[2016-05-02 15:15:52] INFO: Node - fc_8 has shape (-1, 4096)\n",
      "[2016-05-02 15:15:52] INFO: Node - dp_8 has shape (-1, 4096)\n",
      "[2016-05-02 15:15:55] INFO: Node - fc_9 has shape (-1, 19200)\n",
      "[2016-05-02 15:15:55] INFO: Node - dp_9 has shape (-1, 19200)\n",
      "[2016-05-02 15:15:55] INFO: Node - rs_10 has shape (-1, 64, 15, 20)\n",
      "[2016-05-02 15:15:55] INFO: Node - up_11 has shape (-1, 64, 30, 40)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_11 has shape (-1, 512, 30, 40)\n",
      "[2016-05-02 15:15:55] INFO: Node - concat_11 has shape (-1, 1024, 30, 40)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_12 has shape (-1, 512, 30, 40)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_13 has shape (-1, 512, 30, 40)\n",
      "[2016-05-02 15:15:55] INFO: Node - up_14 has shape (-1, 512, 60, 80)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_14 has shape (-1, 256, 60, 80)\n",
      "[2016-05-02 15:15:55] INFO: Node - ls_pool_1 has shape (-1, 96, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - ls_conv_2 has shape (-1, 64, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - ls_conv_3 has shape (-1, 64, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - ls_pool_3 has shape (-1, 64, 60, 80)\n",
      "[2016-05-02 15:15:55] INFO: Node - ls_conv_4 has shape (-1, 64, 60, 80)\n",
      "[2016-05-02 15:15:55] INFO: Node - concat_14 has shape (-1, 576, 60, 80)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_15_b has shape (-1, 256, 60, 80)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_16 has shape (-1, 256, 60, 80)\n",
      "[2016-05-02 15:15:55] INFO: Node - up_17 has shape (-1, 256, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_17 has shape (-1, 128, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - ms_pool_1 has shape (-1, 96, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - ms_conv_2 has shape (-1, 64, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - ms_conv_3 has shape (-1, 64, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - ms_conv_4 has shape (-1, 64, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - concat_17 has shape (-1, 320, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_18 has shape (-1, 128, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_19 has shape (-1, 128, 120, 160)\n",
      "[2016-05-02 15:15:55] INFO: Node - up_20 has shape (-1, 128, 240, 320)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_20 has shape (-1, 64, 240, 320)\n",
      "[2016-05-02 15:15:55] INFO: Node - concat_20 has shape (-1, 128, 240, 320)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_21 has shape (-1, 64, 240, 320)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_22 has shape (-1, 64, 240, 320)\n",
      "[2016-05-02 15:15:55] INFO: Node - conv_23_with_bias has shape (-1, 1, 240, 320)\n",
      "[2016-05-02 15:15:55] INFO: Node - loss has shape (1,)\n",
      "[2016-05-02 15:15:55] INFO: Node - mse has shape (1,)\n",
      "[2016-05-02 15:16:01] INFO: Graph - Invoking Theano compiler\n",
      "[2016-05-02 15:16:17] INFO: Optimizer - Compilation finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastianschlecht/anaconda2/envs/deep/lib/python2.7/site-packages/ipykernel/__main__.py:128: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "/home/sebastianschlecht/anaconda2/envs/deep/lib/python2.7/site-packages/ipykernel/__main__.py:129: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016-05-02 15:16:40] INFO: Optimizer - Training score at iteration 20: {'loss': array(0.5783646106719971, dtype=float32), 'mse': array(0.7605028748512268, dtype=float32)}\n",
      "[2016-05-02 15:17:05] INFO: Optimizer - Training score at iteration 40: {'loss': array(0.764371931552887, dtype=float32), 'mse': array(0.8742836713790894, dtype=float32)}\n",
      "[2016-05-02 15:17:30] INFO: Optimizer - Training score at iteration 60: {'loss': array(2.120760202407837, dtype=float32), 'mse': array(1.4562829732894897, dtype=float32)}\n",
      "[2016-05-02 15:17:55] INFO: Optimizer - Training score at iteration 80: {'loss': array(0.3378673791885376, dtype=float32), 'mse': array(0.5812636017799377, dtype=float32)}\n",
      "[2016-05-02 15:18:20] INFO: Optimizer - Training score at iteration 100: {'loss': array(0.9069490432739258, dtype=float32), 'mse': array(0.952338695526123, dtype=float32)}\n",
      "[2016-05-02 15:18:45] INFO: Optimizer - Training score at iteration 120: {'loss': array(1.7418804168701172, dtype=float32), 'mse': array(1.3198031187057495, dtype=float32)}\n",
      "[2016-05-02 15:19:10] INFO: Optimizer - Training score at iteration 140: {'loss': array(1.2059025764465332, dtype=float32), 'mse': array(1.0981359481811523, dtype=float32)}\n",
      "[2016-05-02 15:19:35] INFO: Optimizer - Training score at iteration 160: {'loss': array(0.884172260761261, dtype=float32), 'mse': array(0.9403043389320374, dtype=float32)}\n",
      "[2016-05-02 15:20:00] INFO: Optimizer - Training score at iteration 180: {'loss': array(0.5859044790267944, dtype=float32), 'mse': array(0.7654439806938171, dtype=float32)}\n",
      "[2016-05-02 15:20:25] INFO: Optimizer - Training score at iteration 200: {'loss': array(1.3172154426574707, dtype=float32), 'mse': array(1.1477000713348389, dtype=float32)}\n",
      "[2016-05-02 15:20:50] INFO: Optimizer - Training score at iteration 220: {'loss': array(0.37584367394447327, dtype=float32), 'mse': array(0.6130608916282654, dtype=float32)}\n",
      "[2016-05-02 15:21:15] INFO: Optimizer - Training score at iteration 240: {'loss': array(1.0903443098068237, dtype=float32), 'mse': array(1.044195532798767, dtype=float32)}\n",
      "[2016-05-02 15:21:40] INFO: Optimizer - Training score at iteration 260: {'loss': array(0.6511337161064148, dtype=float32), 'mse': array(0.8069285750389099, dtype=float32)}\n",
      "[2016-05-02 15:22:05] INFO: Optimizer - Training score at iteration 280: {'loss': array(1.0472750663757324, dtype=float32), 'mse': array(1.023364543914795, dtype=float32)}\n",
      "[2016-05-02 15:22:30] INFO: Optimizer - Training score at iteration 300: {'loss': array(1.1949481964111328, dtype=float32), 'mse': array(1.0931367874145508, dtype=float32)}\n",
      "[2016-05-02 15:22:55] INFO: Optimizer - Training score at iteration 320: {'loss': array(1.3097201585769653, dtype=float32), 'mse': array(1.1444300413131714, dtype=float32)}\n",
      "[2016-05-02 15:23:20] INFO: Optimizer - Training score at iteration 340: {'loss': array(1.5976548194885254, dtype=float32), 'mse': array(1.2639837265014648, dtype=float32)}\n",
      "[2016-05-02 15:23:45] INFO: Optimizer - Training score at iteration 360: {'loss': array(0.3841850161552429, dtype=float32), 'mse': array(0.61982661485672, dtype=float32)}\n",
      "[2016-05-02 15:24:09] INFO: Optimizer - Training score at iteration 380: {'loss': array(1.6451301574707031, dtype=float32), 'mse': array(1.2826262712478638, dtype=float32)}\n",
      "[2016-05-02 15:24:34] INFO: Optimizer - Training score at iteration 400: {'loss': array(0.7402889728546143, dtype=float32), 'mse': array(0.8604004979133606, dtype=float32)}\n",
      "[2016-05-02 15:24:59] INFO: Optimizer - Training score at iteration 420: {'loss': array(1.1643604040145874, dtype=float32), 'mse': array(1.0790553092956543, dtype=float32)}\n",
      "[2016-05-02 15:25:24] INFO: Optimizer - Training score at iteration 440: {'loss': array(0.0958285853266716, dtype=float32), 'mse': array(0.30956190824508667, dtype=float32)}\n",
      "[2016-05-02 15:25:49] INFO: Optimizer - Training score at iteration 460: {'loss': array(0.8436220288276672, dtype=float32), 'mse': array(0.9184890389442444, dtype=float32)}\n",
      "[2016-05-02 15:26:14] INFO: Optimizer - Training score at iteration 480: {'loss': array(0.7404022812843323, dtype=float32), 'mse': array(0.8604663014411926, dtype=float32)}\n",
      "[2016-05-02 15:26:39] INFO: Optimizer - Training score at iteration 500: {'loss': array(2.559889793395996, dtype=float32), 'mse': array(1.5999655723571777, dtype=float32)}\n",
      "[2016-05-02 15:27:13] INFO: Optimizer - Mean loss values for validation at iteration 500 is: {'loss': 0.68710321, 'mse': 0.80551654}\n",
      "[2016-05-02 15:27:38] INFO: Optimizer - Training score at iteration 520: {'loss': array(0.6032447218894958, dtype=float32), 'mse': array(0.776688277721405, dtype=float32)}\n",
      "[2016-05-02 15:28:03] INFO: Optimizer - Training score at iteration 540: {'loss': array(0.9791280627250671, dtype=float32), 'mse': array(0.9895089864730835, dtype=float32)}\n",
      "[2016-05-02 15:28:28] INFO: Optimizer - Training score at iteration 560: {'loss': array(0.5308501124382019, dtype=float32), 'mse': array(0.7285946011543274, dtype=float32)}\n",
      "[2016-05-02 15:28:52] INFO: Optimizer - Training score at iteration 580: {'loss': array(1.5558918714523315, dtype=float32), 'mse': array(1.2473539113998413, dtype=float32)}\n",
      "[2016-05-02 15:29:17] INFO: Optimizer - Training score at iteration 600: {'loss': array(0.9016780853271484, dtype=float32), 'mse': array(0.9495673179626465, dtype=float32)}\n",
      "[2016-05-02 15:29:42] INFO: Optimizer - Training score at iteration 620: {'loss': array(0.9133573770523071, dtype=float32), 'mse': array(0.9556972980499268, dtype=float32)}\n",
      "[2016-05-02 15:30:07] INFO: Optimizer - Training score at iteration 640: {'loss': array(0.802776038646698, dtype=float32), 'mse': array(0.8959776759147644, dtype=float32)}\n",
      "[2016-05-02 15:30:32] INFO: Optimizer - Training score at iteration 660: {'loss': array(1.4051069021224976, dtype=float32), 'mse': array(1.1853721141815186, dtype=float32)}\n",
      "[2016-05-02 15:30:57] INFO: Optimizer - Training score at iteration 680: {'loss': array(1.1321169137954712, dtype=float32), 'mse': array(1.0640097856521606, dtype=float32)}\n",
      "[2016-05-02 15:31:22] INFO: Optimizer - Training score at iteration 700: {'loss': array(0.23309127986431122, dtype=float32), 'mse': array(0.48279526829719543, dtype=float32)}\n",
      "[2016-05-02 15:31:47] INFO: Optimizer - Training score at iteration 720: {'loss': array(1.1508630514144897, dtype=float32), 'mse': array(1.0727828741073608, dtype=float32)}\n",
      "[2016-05-02 15:32:12] INFO: Optimizer - Training score at iteration 740: {'loss': array(0.7344831228256226, dtype=float32), 'mse': array(0.8570199012756348, dtype=float32)}\n",
      "[2016-05-02 15:32:36] INFO: Optimizer - Training score at iteration 760: {'loss': array(0.3547705411911011, dtype=float32), 'mse': array(0.595626175403595, dtype=float32)}\n",
      "[2016-05-02 15:33:01] INFO: Optimizer - Training score at iteration 780: {'loss': array(1.0537148714065552, dtype=float32), 'mse': array(1.0265060663223267, dtype=float32)}\n",
      "[2016-05-02 15:33:26] INFO: Optimizer - Training score at iteration 800: {'loss': array(1.5725181102752686, dtype=float32), 'mse': array(1.2540007829666138, dtype=float32)}\n",
      "[2016-05-02 15:33:51] INFO: Optimizer - Training score at iteration 820: {'loss': array(0.6141523718833923, dtype=float32), 'mse': array(0.7836787104606628, dtype=float32)}\n",
      "[2016-05-02 15:34:16] INFO: Optimizer - Training score at iteration 840: {'loss': array(0.31400036811828613, dtype=float32), 'mse': array(0.5603573322296143, dtype=float32)}\n",
      "[2016-05-02 15:34:41] INFO: Optimizer - Training score at iteration 860: {'loss': array(1.0691279172897339, dtype=float32), 'mse': array(1.0339863300323486, dtype=float32)}\n",
      "[2016-05-02 15:35:06] INFO: Optimizer - Training score at iteration 880: {'loss': array(0.3836820721626282, dtype=float32), 'mse': array(0.6194207668304443, dtype=float32)}\n",
      "[2016-05-02 15:35:31] INFO: Optimizer - Training score at iteration 900: {'loss': array(0.4095643162727356, dtype=float32), 'mse': array(0.6399721503257751, dtype=float32)}\n",
      "[2016-05-02 15:35:56] INFO: Optimizer - Training score at iteration 920: {'loss': array(1.011916160583496, dtype=float32), 'mse': array(1.0059404373168945, dtype=float32)}\n",
      "[2016-05-02 15:36:21] INFO: Optimizer - Training score at iteration 940: {'loss': array(0.5959439873695374, dtype=float32), 'mse': array(0.7719740867614746, dtype=float32)}\n",
      "[2016-05-02 15:36:46] INFO: Optimizer - Training score at iteration 960: {'loss': array(1.336908221244812, dtype=float32), 'mse': array(1.1562474966049194, dtype=float32)}\n",
      "[2016-05-02 15:37:10] INFO: Optimizer - Training score at iteration 980: {'loss': array(1.4332820177078247, dtype=float32), 'mse': array(1.1971975564956665, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    batch_size = 4\n",
    "    chunk_size = 10*batch_size\n",
    "    transfer_shape = ((chunk_size, 3, 240, 320), (chunk_size, 240, 320))\n",
    "\n",
    "    g = build_u_graph()\n",
    "\n",
    "    # Build the training pipeline\n",
    "    db_loader = H5DBLoader(\"db\", ((chunk_size, 3, 480, 640), (chunk_size, 1, 480, 640)), config={\n",
    "        \"db\": \"/media/data/depth_data/nyu_depth_combined_vnet2.hdf5\",\n",
    "        # \"db\": '../data/nyu_depth_unet_large.hdf5',\n",
    "        \"key_data\": \"images\",\n",
    "        \"key_label\": \"depths\",\n",
    "        \"chunk_size\": chunk_size\n",
    "    })\n",
    "    transformer = Transformer(\"tr\", transfer_shape, config={\n",
    "        # Pixel & channelwise mean \n",
    "        \"mean_file\": \"/media/data/depth_data/nyu_depth_combined_vnet2.npy\",\n",
    "        # 1 / sqrt(var(data))\n",
    "        \"std_norm\": 1.0 / 72.240064849107824,\n",
    "        \"pred_std_norm\": 1.0 / 1.6549985\n",
    "    })\n",
    "    optimizer = Optimizer(\"opt\", g, transfer_shape, config={\n",
    "        \"batch_size\":  batch_size,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.0005,\n",
    "        \"print_freq\": 20,\n",
    "        \"save_freq\": 60000,\n",
    "        \"lr_policy\": \"step\",\n",
    "        \"step_size\": 60000,\n",
    "        \"save_prefix\": \"/media/data/depth_models/vnet3_5_paths\"\n",
    "    })\n",
    "\n",
    "    p = Pipeline(config={\n",
    "        \"validation_frequency\": 50,\n",
    "        \"cycles\": 12500\n",
    "    })\n",
    "    p.add(db_loader)\n",
    "    p.add(transformer)\n",
    "    p.add(optimizer)\n",
    "    p.run()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "l = np.array([s[\"loss\"] for s in optimizer.losses])\n",
    "e = np.array([s[\"mse\"] for s in optimizer.losses])\n",
    "print l.mean()\n",
    "plt.plot(l)\n",
    "plt.show()\n",
    "plt.plot(e)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py, numpy as np\n",
    "f = h5py.File(\"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.hdf5\")\n",
    "b = int(f[\"images\"].shape[0] * 0.9)\n",
    "images = np.array(f[\"images\"][0:100])\n",
    "depths = np.array(f[\"depths\"][0:100])\n",
    "print images.shape\n",
    "mean = np.load(\"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = build_u_graph()\n",
    "g.load_weights(\"/data/depth_models/vnet3_5_paths_iter_15000.zip\")\n",
    "g.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from deepgraph.nn.core import Dropout\n",
    "\n",
    "plot = True\n",
    "idx = 0\n",
    "diffs = []\n",
    "Dropout.set_dp_off()\n",
    "for image in images:\n",
    "    tmp = image.astype(np.float32)\n",
    "    tmp -= mean\n",
    "    tmp *= 1.0 / 72.240064849107824\n",
    "    \n",
    "    res = g.infer([tmp.reshape((1,3,240,320))])[\"conv_23_with_bias\"]\n",
    "    res = res.squeeze()\n",
    "    res = np.exp(res / (1.0 / 1.6549985)) \n",
    "    depth = depths[idx]\n",
    "    if plot and idx % 4 == 0:\n",
    "        \n",
    "        plt.imshow(image.transpose((1,2,0)).astype(np.uint8))\n",
    "        plt.show()\n",
    "        plt.imshow(depth)\n",
    "        plt.show()\n",
    "        plt.imshow(res)\n",
    "        plt.show()\n",
    "        print \"RMSE: \" + str(np.sqrt(np.mean((res-depth)**2)))\n",
    "    diffs.append(res - depth)\n",
    "    \n",
    "    idx += 1\n",
    "    \n",
    "diffs = np.array(diffs)\n",
    "rmse = np.sqrt(np.mean(diffs ** 2))\n",
    "print \"Accumulated RMSE: \" + str(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer random images from the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib, cStringIO\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from deepgraph.nn.core import Dropout\n",
    "\n",
    "\n",
    "Dropout.set_dp_off()\n",
    "\n",
    "\n",
    "urls = [\n",
    "    'http://www.hotel-im-wasserturm.de/fileadmin/_processed_/csm_Deluxe_Doppelzimmer_2_6455fa52be.jpg',\n",
    "    'http://hotel-airport-zuerich.dorint.com/fileadmin/_processed_/csm_ZRH_Zimmer_DZ_04_bc3a12f87e.jpg',\n",
    "    'http://www.erlebnis-erlensee.de/images/bilder/partyservice/schnitzel.jpg',\n",
    "    'http://www.hankewitz.com/wp-content/uploads/2012/12/bolognese21.jpg'\n",
    "       ]\n",
    "for URL in urls:\n",
    "    file = cStringIO.StringIO(urllib.urlopen(URL).read())\n",
    "    img = Image.open(file)\n",
    "\n",
    "    rs = img.resize((320,240))\n",
    "    mean = np.load(\"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.npy\")\n",
    "    arr = np.array(rs).transpose((2,0,1)).astype(np.float32)\n",
    "    arr -= mean\n",
    "    arr *= 1.0 / 72.240064849107824\n",
    "    res = g.infer([arr.reshape((1,3,240,320))])[\"conv_23_with_bias\"]\n",
    "    res = np.exp(res)\n",
    "    # Montage to manipulate color scale\n",
    "    mmax = 8.0\n",
    "    # res[0,0,0,0] = 0.0\n",
    "    # res[0,0,0,1] = mmax\n",
    "    plt.imshow(rs)\n",
    "    plt.show()\n",
    "    plt.imshow(res.squeeze().clip(0.0,mmax))\n",
    "    plt.show()\n",
    "\n",
    "    print res.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer image from food data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib, cStringIO\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from deepgraph.nn.core import Dropout\n",
    "import numpy as np\n",
    "\n",
    "Dropout.set_dp_off()\n",
    "\n",
    "data = np.load('/home/ga29mix/nashome/data/kinectv2_2016_03_30_13_33_25_bgr.npy')[:,:,0:3]\n",
    "dd = np.load('/home/ga29mix/nashome/data/kinectv2_2016_03_30_13_33_25_depth.npy')\n",
    "\n",
    "img = Image.fromarray(np.array(data[:,:,::-1]))\n",
    "rs = img.resize((320,240))\n",
    "mean = np.load(\"/home/ga29mix/nashome/data/nyu_depth_v2_combined_50.npy\")\n",
    "arr = np.array(rs).transpose((2,0,1)).astype(np.float32)\n",
    "arr -= mean\n",
    "arr *= 1.0 / 72.240064849107824\n",
    "res = g.infer([arr.reshape((1,3,240,320))])[\"conv_23_with_bias\"]\n",
    "res = np.exp(res)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.imshow(res.squeeze())\n",
    "plt.show()\n",
    "plt.imshow(dd)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
